\documentclass[runningheads]{llncs}

\usepackage[ruled,vlined,linesnumbered,norelsize]{algorithm2e}
\usepackage{amsmath,amssymb}
\usepackage[T1]{fontenc} % font encoding; recommended by LNCS template
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[misc]{ifsym} % letter symbol to mark corresponding author
\usepackage{pifont}
\usepackage{siunitx}
\usepackage{subcaption}
\usepackage{tablefootnote}
\usepackage{tabularx}
\usepackage[table]{xcolor}
\usepackage{tikz} % has to be loaded after "xcolor" here, else "option clash"

\newcommand{\cmark}{\ding{51}} % symbols for table comparing benchmarking approaches
\newcommand{\xmark}{\ding{55}}

\DontPrintSemicolon % configuration for algorithm2e
\def\NlSty#1{\textnormal{\fontsize{8}{10}\selectfont{}#1}}
\SetKwSty{texttt}
\SetCommentSty{emph}

\renewcommand\UrlFont{\color{blue}\rmfamily} % link coloring according to LNCS template

\begin{document}

\title{Active Learning for SAT Solver Benchmarking}

\author{
	Tobias Fuchs\orcidID{0000-0001-9727-2878} (\Letter) \and \\
	Jakob Bach\orcidID{0000-0003-0301-2798} \and \\
	Markus Iser\orcidID{0000-0003-2904-232X}
}

\institute{
	Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany \\
	\email{info@tobiasfuchs.de, jakob.bach@kit.edu, markus.iser@kit.edu}
}

\authorrunning{T. Fuchs et al.} % according to sample paper of template: first names are abbreviated; if there are more than two authors, 'et al.' is used.

\maketitle

\begin{abstract}
  Benchmarking is one of the most important phases when developing algorithms.
  This also applies to solutions of the SAT (propositional satisfiability) problem.
  While the field of SAT solver benchmarking is well established, traditional benchmark selection approaches do not optimize for benchmark runtime.
  Benchmark selection chooses representative instances from a pool of instances such that they reliably discriminate SAT solvers based on their runtime.
  In this paper, we present a dynamic benchmark selection approach based on active learning.
  Our approach predicts the rank of a new solver among its competitors with minimum running time and maximum rank prediction accuracy.
  We evaluated this approach using the Anniversary Track dataset from the 2022 SAT Competition.
  Our selection approach can predict the rank of a new solver after about \SI{10}{\%} of the time it would take to run the solver on all instances of this dataset, with a prediction accuracy of about \SI{92}{\%}. 
  In this paper, we also discuss the importance of instance families in the selection process. 
  Overall, our tool provides a reliable way for SAT solver engineers to efficiently determine the performance of a new SAT solver.

  \keywords{Propositional satisfiability \and Benchmarking \and Active learning}
\end{abstract}


\section{Introduction}
\label{sec:intro}

One of the main phases of algorithm engineering is benchmarking.
This is also true for the pro\-po\-si\-tio\-nal sat\-is\-fia\-bi\-li\-ty problem (SAT), the archetypal $\mathcal{NP}$-complete problem.
While state-of-the-art solvers embark upon more and more problem instances, SAT solver evaluation is steadily getting more costly.
%JB: not sure about the "while" here -- is the message that (1) evaluation gets costly *since* solvers have to solve more instances or is it that (2) solvers are threoretically able to solve more instances, but evaluation gets costlier anyway
Competitive SAT solvers are the result of extensive experimentation and a variety of ideas and considerations~\cite{FroleyksHIJS21,sat2022}.
While the field of SAT solver benchmarking is well established, traditional benchmark selection approaches do not optimize for benchmark runtime.
Instead, the primary goal of traditional approaches is to select a representative set of instances for ranking solvers with a scoring scheme~\cite{Gelder11,HoosKSS13}.
SAT~Competitions typically use the widely adopted PAR-2 score, i.e, the average runtime with a penalty of $2 \tau$ for timeouts with time limit~$\tau$~\cite{FroleyksHIJS21}.

In this paper, we present a novel benchmark selection approach based on active learning.
Our approach is able to predict the rank of a new solver with high accuracy in only a fraction of the time needed to evaluate the full benchmark.
The problem we solve is specified in Definition~\ref{def:new-solver-problem}.

\begin{definition}[New-Solver Problem]
	Given a pool of instances~$\mathcal{I}$, solvers~$\mathcal{A}$, and runtimes~$r\!: \mathcal{A} \times \mathcal{I} \rightarrow \left[0, \tau\right]$ with time limit $\tau$, maximize the confidence in predicting the rank of a new solver $\hat{a} \notin \mathcal{A}$ while minimizing the total benchmark runtime by incrementally selecting instances in $\mathcal{I}$ to evaluate $\hat{a}$.
	\label{def:new-solver-problem}
\end{definition}

Note that our scenario assumes that we know the runtimes of all solvers, except the new one, on all instances.
One could also imagine a collaborative filtering scenario, where runtimes are only partially known~\cite{misir2017data,misir2017alors}.

Our approach satisfies several desirable criteria for benchmarking:
It outputs a \emph{ranking} to compare the new solver against the existing set of solvers.
For this ranking, we do not even need to predict exact solver runtimes, which is trickier.
We optimize the \emph{runtime} that our strategy needs to arrive at its conclusion.
We use problem-instance and runtime \emph{features}.
Moreover, we select instances \emph{non-randomly} and \emph{incrementally}.
In particular, we take runtime information from already done experiments into account when choosing the next.
By doing so, we can control the properties of the benchmarking approach such as its required runtime.
Rather than solely outputting a binary classification, i.e, the new solver is worse than an existing solver, we provide a \emph{scoring} function that shows by which margin a solver is worse and how similar it is to existing solvers.
Our approach is \emph{scalable} in that it ranks a new solver $\hat{a}$ among any number of known solvers $\mathcal{A}$.
In particular, we only subsample the benchmark once instead of comparing pairwise against each other solver~\cite{MatriconAFSH21}.
Since a new solver idea is rarely best on the first try, it is desired to obtain a solver ranking fast.
In this way, a new solver idea can be discarded if it performs poorly across the board or may be further tweaked if it shows promising results at least in some cases.

We evaluate our approach with the SAT Competition~2022 Anniversary Track benchmark~\cite{sat2022}, consisting of 5355~instances and full runtimes of 28~solvers.
We perform cross-validation by treating each solver as new once and predicting these solvers' PAR-2 rank.
On average, our predictions reach about \SI{92}{\%} accuracy with only about \SI{10}{\%} of the runtime that would be needed to evaluate these solvers on the complete set of instances.

All our source code\footnote{temporary, anonymized version for review: \url{xxx}} and and data\footnote{temporary, anonymized version for review: \url{xxx}} are available on GitHub.


\section{Related Work}

Benchmarking is not only of high interest in many fields but also an active research area on its own.
Recent studies show that benchmark selection is a difficult endeavor for multiple reasons.
Biased benchmarks can easily lead to fallacious interpretations~\cite{abs-2107-07002}.
Also, benchmark selection has many movable parts, e.g., performance measures, aggregation, and handling of missing values.
Questionable research practices might modify these elements a-posteriori to fit expectations, leading to biased results~\cite{NiesslHWCB22}.
In the following, we discuss related work from the areas of static benchmark selection, algorithm configuration, incremental benchmark selection, and active learning.

\subsubsection{Static benchmark selection.}

Benchmark selection is an important issue for competitions, e.g., the SAT Competition.
In such competitions, the organizers define the rules for composing the corresponding benchmarks.
Balint et al. provide an overview of benchmark-selection criteria in different solver competitions~\cite{balint2015overview}.
%JB: not sure if we should provide some details regarding SAT Competition 2022 \cite{sat2022}, previous competitions (if different procedure used; Related-Work section of Matricon et al. (2021) states that) or other competitions (e.g., those in https://www.floc2022.org/floc-olympic-games)
%TF: @Markus maybe you are better at briefly discussing the  SAT Competition 2022 and previous competitions
Manthey and MÃ¶hle find that competition benchmarks might contain redundant instances, and propose a feature-based approach to remove redundancy~\cite{manthey2016better}.
M{\i}s{\i}r presents a feature-based approach to reduce benchmarks by matrix factorization and clustering~\cite{misir2021benchmark}.

Hoos et al. discuss which properties are most desirable when selecting SAT benchmark instances~\cite{HoosKSS13}.
The selection strategy presented is static, i.e., it does not depend on particular solvers to distinguish.
Selection criteria are instance variety to avoid over-fitting, adapted instance hardness (not too easy but also not too hard), and avoiding duplicate instances.
To filter too similar instances, they use a distance-based approach with the SATzilla features~\cite{XuHHL08,features}.
Their approach \emph{ranks} solvers, is \emph{feature-based}, and \emph{scalable} in the sense that the evaluation time scales linearly with more instances and solvers.
Also, it allows \emph{scoring} and thereby comparing solvers.
However, the approach does not optimize for benchmark \emph{runtime} and selects instances \emph{randomly}, apart from constraints on the instance hardness and feature distance.

\begin{table}[tbp]
  \centering
  \begin{tabular}{
    m{0.32\textwidth}
    >{\centering\arraybackslash}m{0.16\textwidth}
    >{\centering\arraybackslash}m{0.16\textwidth}
    >{\centering\arraybackslash}m{0.16\textwidth}
    >{\centering\arraybackslash}m{0.16\textwidth}
  }
    \hline
    Feature & Hoos~\cite{HoosKSS13} & SMAC~\cite{HutterHL11} & Matricon~\cite{MatriconAFSH21} & Our approach \\
    \hline
    Ranking & \cmark & \xmark & \cmark & \cmark \\
    Runtime Minimization & \xmark & \cmark & \cmark & \cmark \\
    % Feature-based & \cmark & \cmark & \cmark & \cmark \\
    Incremental/Non-Random & \xmark & \xmark & \cmark & \cmark \\
    % \tablefootnote{While using a model-based selection strategy to select the next configuration to evaluate, the instance sampling is done randomly.}
    Scoring & \cmark & \xmark & \xmark & \cmark \\
    % \tablefootnote{While they provide a confidence measure, the strategy's output is still only binary, i.e., whether a solver A is better or worse than solver B.}
    Scalability & \cmark & \cmark & \xmark & \cmark \\
    \hline
  \end{tabular}
  ~\\[1em]
  \caption{Support for desired criteria from Section~\ref{sec:intro}.
	The table compares a static benchmark-selection approach~\cite{HoosKSS13}, an algorithm configuration system~\cite{HutterHL11}, an existing active-learning approach~\cite{MatriconAFSH21}, as well as our approach.
  }
  \label{tab:requirements}
\end{table}

\subsubsection{Algorithm configuration.}

Further related work can be found within the field of algorithm configuration~\cite{HoosHL21,Stutzle0P22}, e.g., the configuration system SMAC~\cite{HutterHL11}.
There, the goal is to tune SAT solvers for a given sub-domain of problem instances.
Although this task is different from our goal, e.g., we do not need to navigate configuration space, there are similarities to our approach as well.
For example, SMAC also employs an iterative, model-based selection procedure, though for configurations rather than instances.
SMAC's configuration sampling has also proven to be better than random sampling in all tested scenarios and has even proven to be significantly better in most cases.
While this approach considers the configuration's \emph{runtime}, is \emph{feature-based}, and \emph{scales} by having a fixed time budget, it does not satisfy the other three criteria listed in Table~\ref{tab:requirements}.
First, an algorithm configurator cannot be used to \emph{rank} a new solver since it only looks at promising solvers/configurations rather than the overall average performance.
Second, while using a model-based selection strategy to sample configurations, instance sampling is done \emph{randomly}, i.e., without building a model over instances.
And third, only few configurations within their approach are \emph{scored} since algorithm configuration seeks to find solemnly the best performing configuration.

\subsubsection{Incremental benchmark selection.}

Matricon~et~al. present an incremental benchmark selection approach~\cite{MatriconAFSH21}.
Their \emph{per-set efficient algorithm selection problem} (PSEAS) is similar to our \emph{New-Solver Problem} (cf.~Definition~\ref{def:new-solver-problem}).
Given a pair of SAT solvers, they iteratively select a subset of instances until the desired confidence level is reached to decide which of the two solvers is better.
The selection of instances is dependent on the choice of the solvers to distinguish.
They calculate a scoring metric for all unselected instances, run the experiment with the highest score, and update the confidence.
Most of the proposed strategies in~\cite{MatriconAFSH21} are instance-based, i.e., no model of the solver runtimes is built.
Regarding our desired criteria of benchmark selection (Table~\ref{tab:requirements}), the aforementioned approach \emph{ranks} solvers, optimizes for \emph{runtime}, is \emph{feature-based}, and uses \emph{incremental non-random} sampling.
However, the approach only compares solvers binarily rather than providing a \emph{scoring}.
Thus, it is not clear how similar two given solvers are or on which instances they behave similarly.
Moreover, a major shortcoming is the lacking \emph{scalability} with the number of solvers.
Comparing only pairs of solvers, evaluating a new solver requires sampling several benchmarks.
In contrast, our approach allows comparing a new solver against a set of existing solvers with one benchmark.

% MI: move to approach if needed there:
%For the scoring metric, they discuss (1) a random strategy as a baseline, (2) a discrimination strategy based on the work of Gent et al.~\cite{GentHJKMNN14}, (3) a variance-based strategy (select instance with the highest variance in its distribution of running times), (4) an information-gain-based method, and (5) a feature-distance-based strategy.
%To update the confidence level after each experiment, they discuss using (1) a fixed subset size, (2) a Wilcoxon test, and (3) a distribution-based method.
%Their experiments show that random instance selection with a Wilcoxon test as the stopping criterion performs consistently well across different data sets in the sense that it belongs to the Pareto front or is close to it in all cases.
%JB: in our evaluation, we should state whether our results are consistent with this or not
%TF: will do

\subsubsection{Active learning (AL).}

%\begin{figure}[tbp!]
%  \centering
%  \begin{tabular}[c]{ccc}
%  \begin{subfigure}[b]{0.25\textwidth}
%  \centering
%  \resizebox{!}{2.75cm}{
%  \begin{tikzpicture}
%  \draw[fill=black]  (-2.8,3.7) ellipse (0.1 and 0.1);
%  \draw[fill=black]  (-2.6,3.4) arc (0:180:0.2);
%  \node[right] at (-5.9,3.6) {\large Agent (Solver)};
%  \draw[fill=white]  (-3.3,2.8) rectangle (-2.5,2.2);
%  \draw[fill=white]  (-3.2,2.7) rectangle (-2.4,2.1);
%  \draw[fill=white]  (-3.1,2.6) rectangle (-2.3,2);
%  \node[right] at (-5.9,2.4) {\large Train. Data};
%  \node (v1) at (-2.8,3.4) {};
%  \node (v2) at (-2.8,2.8) {};
%  \draw[->]  (v1) edge (v2);
%  \node[right] at (-2.8,3.1) {\large $x, y$};
%  \node (v3) at (-2.8,2) {};
%  \node (v4) at (-2.8,1.4) {};
%  \draw[->]  (v3) edge (v4);
%  \draw  (-3.1,1.4) rectangle (-2.5,1);
%  \draw  (-2.4,3.2) rectangle (-2.4,3.2);
%  \node[right] at (-5.9,1.2) {\large Learning Alg.};
%  \node (v5) at (-2.8,1) {};
%  \node (v6) at (-2.8,0.4) {};
%  \draw[->]  (v5) edge (v6);
%  \node[right] at (-5.9,0.25) {\large Approx. Func.};
%  \node at (-2.8,0.25) {\large $f$};
%  \end{tikzpicture}
%  }
%  \caption{Passive}
%  \label{fig:passive}
%  \end{subfigure}
%  &
%  \begin{subfigure}[b]{0.35\textwidth}
%  \centering
%  \resizebox{!}{2.75cm}{
%  \begin{tikzpicture}
%  \draw[fill=black]  (-2.8,3.7) ellipse (0.1 and 0.1);
%  \draw[fill=black]  (-2.6,3.4) arc (0:180:0.2);
%  \node[right] at (-5.9,3.6) {\large Agent (Solver)};
%  \draw[fill=white]  (-3.3,2.8) rectangle (-2.5,2.2);
%  \draw[fill=white]  (-3.2,2.7) rectangle (-2.4,2.1);
%  \draw[fill=white]  (-3.1,2.6) rectangle (-2.3,2);
%  \node[right] at (-5.9,2.4) {\large Train. Data};
%  \node (v1) at (-2.8,3.4) {};
%  \node (v2) at (-2.8,2.8) {};
%  \draw[->]  (v1) edge (v2);
%  \node[right] at (-2.8,3.1) {\large $x, y$};
%  \node (v3) at (-2.8,2) {};
%  \node (v4) at (-2.8,1.4) {};
%  \draw[->]  (v3) edge (v4);
%  \draw  (-3.1,1.4) rectangle (-2.5,1);
%  \draw  (-2.4,3.2) rectangle (-2.4,3.2);
%  \node[right] at (-5.9,1.2) {\large Learning Alg.};
%  \node (v5) at (-2.8,1) {};
%  \node (v6) at (-2.8,0.4) {};
%  \draw[->]  (v5) edge (v6);
%  \node[right] at (-5.9,0.25) {\large Approx. Func.};
%  \node at (-2.8,0.25) {\large $f$};
%  \node[right] at (-1.2,2.4) {\large Pool};
%  \draw[->, densely dashed] (-2.4,1.2) arc (-90:0:0.8);
%  \draw[->, densely dashed] (-1.6,2.8) arc (0:90:0.8);
%  \node[right] at (-1.6,3.135) {\large $x, ?$};
%  \draw  (-1.6,2.4) ellipse (0.4 and 0.3);
%  \draw[fill=black]  (-1.725,2.5) ellipse (0.07 and 0.07);
%  \draw[fill=black]  (-1.625,2.25) ellipse (0.07 and 0.07);
%  \draw[fill=black]  (-1.425,2.4) ellipse (0.07 and 0.07);
%  \end{tikzpicture}
%  }
%  \caption{Active Pool-based}
%  \label{fig:activepool}
%  \end{subfigure}
%  &
%  \begin{subfigure}[b]{0.35\textwidth}
%  \centering
%  \resizebox{!}{2.75cm}{
%  \begin{tikzpicture}
%  \draw[fill=black]  (-2.8,3.7) ellipse (0.1 and 0.1);
%  \draw[fill=black]  (-2.6,3.4) arc (0:180:0.2);
%  \node[right] at (-5.9,3.6) {\large Agent (Solver)};
%  \draw[fill=white]  (-3.3,2.8) rectangle (-2.5,2.2);
%  \draw[fill=white]  (-3.2,2.7) rectangle (-2.4,2.1);
%  \draw[fill=white]  (-3.1,2.6) rectangle (-2.3,2);
%  \node[right] at (-5.9,2.4) {\large Train. Data};
%  \node (v1) at (-2.8,3.4) {};
%  \node (v2) at (-2.8,2.8) {};
%  \draw[->]  (v1) edge (v2);
%  \node[right] at (-2.8,3.1) {\large $x, y$};
%  \node (v3) at (-2.8,2) {};
%  \node (v4) at (-2.8,1.4) {};
%  \draw[->]  (v3) edge (v4);
%  \draw  (-3.1,1.4) rectangle (-2.5,1);
%  \draw  (-2.4,3.2) rectangle (-2.4,3.2);
%  \node[right] at (-5.9,1.2) {\large Learning Alg.};
%  \node (v5) at (-2.8,1) {};
%  \node (v6) at (-2.8,0.4) {};
%  \draw[->]  (v5) edge (v6);
%  \node[right] at (-5.9,0.25) {\large Approx. Func.};
%  \node at (-2.8,0.25) {\large $f$};
%  \node[right, label={[align=left]\large Gen.\\\large Model}] at (-0.6,1.85) {};
%  \node[right] at (-1.3,3.135) {\large $x, ?$};
%  \draw[->, densely dashed] (-2.4,1.2) arc (-90:90:1.2);
%  \end{tikzpicture}
%  }
%  \caption{Active Synthesis-based}
%  \label{fig:activesynth}
%  \end{subfigure}
%  \end{tabular}
%  
%  \caption{Types of Learning (Inspired by Rubens~et.al.~\cite{RubensESK15})}
%  \label{fig:learning}
%\end{figure}

The posed \emph{New-Solver Problem} has stark similarities to the well-studied field of active learning AL within recommender systems, especially the \emph{new-user problem}~\cite{RubensESK15}.
On the one hand, we want to maximize the utility an instance provides to our model and, on the other hand, minimize the cost (CPU time) that is associated with its acquisition.
In contrast to traditional passive machine-learning methods with given instance labels, active learning allows for selecting instances for which to acquire labels.
AL algorithms can be categorized into \emph{synthesis-bas\-ed} \cite{0001AEMN22,GarzonMG22,2019gaal} and \emph{pool-bas\-ed} approaches \cite{GolbandiKL11,HarpaleY08,KorenBV09}.%SinhaED19,distribAL,TongK01,KapoorGUD07,KornerW06,MelvilleM04}.
While synthesis-based methods generate instances for labeling, pool-based methods rely on a fixed set of unlabeled instances from which to sample.

Recent synthesis-based methods within the field of SAT solving show how to generate problem instances with desired properties.
This goal is, however, orthogonal to ours~\cite{0001AEMN22,GarzonMG22}.
While those approaches want to generate problem instances on which a solver is good or bad, we want to predict whether a solver is good or bad on an existing benchmark.
Volpato and Guangyan use pool-based AL to learn an instance-specific algorithm selector~\cite{volpato2019active}.
Rather than benchmarking a solver's overall performance, the goal is to recommend the best solver out of a set of solvers for each SAT instance.

% MI: here we should limit this to the methods needed for presenting related work for AL in benchmarking:
%JB: should probably just put the citation there where we explain the strategies (in Section 3)
%Pool-based methods can be further refined into \emph{Error-based} \cite{GolbandiKL11,KorenBV09}, \emph{Un\-cer\-tain\-ty-based} \cite{HarpaleY08,KapoorGUD07,TongK01}, \emph{Distribution-based} methods \cite{distribAL,SinhaED19} and \emph{Ensembles} \cite{KornerW06,MelvilleM04}.
%Error-based strategies pick instances that have the most influence on the model parameters.
%Uncertainty-based methods sample instances close to their perceived decision boundary.
%Distribution-based approaches sample instances whose features are unaccounted for amongst the already sampled instances.
%Finally, ensembles are a meta-strategy that combines the predictions of several different active-learning models into a selection choice.


\section{Active Learning for SAT Solver Benchmarking}
\label{sec:main}

In this section, we present the details of our approach.
Section~\ref{sec:main-framework} describes the general framework.
Section~\ref{sec:main-model} specifies the details of the underlying prediction model and describes how we may derive a solver ranking from that model.
We discuss possible criteria for sampling instances in Section~\ref{sec:main-sampling}.
Section~\ref{sec:main-stopping} concludes with possible stopping conditions.


\subsection{Incremental Benchmarking Framework}
\label{sec:main-framework}

Our framework to score a new solver~$\hat{a}$ follows a simple three-step procedure, outlined in Algorithm~\ref{algALBenchmark}:
While the stopping condition is not met (line~1), we select \emph{one} problem instance from the pool of instances with the help of a prediction model~$\mathcal{M}$ (line~2) and evaluate the new solver~$\hat{a}$ on this instance (line~3).
We then use the acquired result to update the prediction model~$\mathcal{M}$ (line~4).
When the stopping criterion is met, we return the predicted score of the new solver~$\hat{a}$ (line~5).

Since the potential runtime of experiments is by magnitudes larger than the model's update time, we only consider incrementing our benchmark by one instance at a time rather than using batches, which is also proposed in current active-learning advances~\cite{SinhaED19,2019gaal}.

\begin{algorithm}
% MI: comment in to rename to framework and remove numbering:
%\SetAlgorithmName{Framework}{framework}{List of Frameworks}
%\renewcommand{\thealgocf}{}
  \caption{Incremental Benchmarking Framework}
  \label{algALBenchmark}

  %\KwIn{Instances $\mathcal{I}$, Solvers $\mathcal{A}$, Runtimes $r\!: \mathcal{A} \times \mathcal{I} \rightarrow \left[0, \tau\right]$}
  \KwIn{Model $\mathcal{M}$,  Solver $\hat{a}$}
  \KwOut{Score of $\hat{a}$}

  \BlankLine

  %$\mathcal{M} \leftarrow \textrm{init model with $\mathcal{A}$, $\mathcal{I}$, $r$, and $\hat{a}$}$
  %$\tilde{\mathcal{I}} \leftarrow \emptyset$

  \While(\tcp*[f]{cf. Section~\ref{sec:main-stopping}}){$\operatorname{not} \, \operatorname{stop}\!\left(\mathcal{M}\right)$}{
    $s \leftarrow \operatorname{selectNextInstance}\!\left(\mathcal{M}\right)$ \tcp*{cf. Section~\ref{sec:main-sampling}}

    $t \leftarrow \operatorname{runExperiment}\!\left(\hat{a},\,  s\right)$  \tcp*{Runs $\hat{a}$ on $s$ with time-out $\tau$}

    %$\tilde{\mathcal{I}} \leftarrow \tilde{\mathcal{I}} \cup \left\lbrace s \right\rbrace$

    $\operatorname{updateModel}\!\left(\mathcal{M},\, s,\, t\right)$ \tcp*{cf. Section~\ref{sec:main-model}}
  }
  
  \Return $\operatorname{predictScore}\!\left(\mathcal{M}\right)$ \tcp*{cf. Section~\ref{sec:main-model}}

  %$\operatorname{score}\!\left(\right) \leftarrow \operatorname{predictScores}\!\left(\mathcal{M}\right)$
  %\KwOut{Return tuple of scoring function $\operatorname{score}\!: \mathcal{A} \cup \left\lbrace \hat{a} \right\rbrace \rightarrow \mathbb{R}$ and the selected instances $\tilde{\mathcal{I}}$.}
\end{algorithm}


\subsection{Solver Model}
\label{sec:main-model}

% MI: describe initialization and update of model


Let benchmark instances $\mathcal{I}$, SAT solvers $\mathcal{A}$, and runtimes $r : \mathcal{A} \times \mathcal{I} \rightarrow \left[0, \tau\right]$ be given. 
We denote the new solver to be ranked by $\hat a \not\in A$ and define $\hat A := A \cup \hat a$ to denote all solvers including the new solver. 
At the core of the model $\mathcal{M}$ is a prediction function $f : \mathcal{\hat A} \times \mathcal{I} \rightarrow \mathbb{R}$ that powers the decisions within the previously mentioned three steps.

To be applicable to $\hat{a}$, we extend \emph{score} by replacing $\gamma(a, e)$ by the model's predictions $f : \mathcal{\hat A} \times \mathcal{I} \rightarrow \left\lbrace 1, \dots, k \right\rbrace$.


\subsubsection{Runtime Transformation.}

We transform the runtimes into discrete runtime classes which are determined on a per instance basis.
For each instance $e \in \mathcal{I}$, we use a single-link hierarchial clustering algorithm to assign the runtimes in $\bigl\{ r(a, e) \mid a \in A \bigr\}$ to one of the $k$ clusters $C_1, \dots, C_k$ such that the fastest runtimes for instance $e$ are in cluster $C_1$ and the slowest are in cluster $C_k$.
For instances with many timeouts, we have to manually fix the clustering results such that all timeouts $\tau$ are in cluster $C_k$.
The runtime transformation function $\gamma_k : {\mathcal{A} \times \mathcal{I}} \rightarrow \left\lbrace 1, \dots, k \right\rbrace$ is then specified as follows.
$$\gamma_k(a, e) = j ~\Leftrightarrow~ r(a, e) \in C_j$$

Given an instance $e \in \mathcal{I}$, a solver $a \in A$ belongs to the $\gamma_k(a, e)$-fastest solvers on instance $e$. 
In our preliminary experiments, we achieved higher accuracy with prediction of such discrete runtime labels than with prediction of raw runtimes. 
The mean squared error of raw runtime prediction was within the same magnitude as the values to be predicted. 
%MI: how much more accurate are we with discrete runtime labels
Research in portfolio solvers has also shown that directly predicting runtimes without any transformation does not work well in practice~\cite{NgokoCT19,CollauttiMMO13}.

To determine solver ranks, we use the transformed runtimes $\gamma_k(a, e)$ in the adapted scoring function $s_k : A \rightarrow [1, 2 \cdot k]$ as follows.
\begin{align}
  s_k(a) := \frac{1}{|\mathcal{I}|} \sum_{e \in \mathcal{I}} \gamma'_k(a, e)
  &&
  \gamma'_k(a, e) := \begin{cases}
  	2 \cdot \gamma_k(a, e) 	& \text{if } \gamma_k(a, e) = k\\
	\gamma_k(a, e)	& \text{otherwise}
  \end{cases}
  \label{eq:rankingeq}
\end{align}

The scoring function $s_k$ induces a ranking among solvers. 
In preliminary experiments, we compared the ranking induced by $s_3$ to the ranking induced by the PAR-2 score which is commonly used in SAT competitions~\cite{FroleyksHIJS21}. 
The Spearman correlation of $s_3$ and PAR-2 ranking is about \SI{0.988} which is very close to the optimal value of 1~\cite{de2016comparing}.

That discretized runtimes are sufficient for discriminating solvers can also be shown by comparing the fraction of solver pairs whose runtimes are significantly different. 
In preliminary experiments, we used a Wilcoxon signed rank test with $\alpha = 0.05$ to compare solver pairs by their runtime vectors.
Within the SAT~Competition~2022 Anniversary track, about \SI{89.95}{\%} of solver pairs are significantly different according to that measure.
The strongly discretized runtimes $\gamma_2$ are sufficient to significantly distinguish about \SI{87.04}{\%} of solver pairs, while $\gamma_3$ is sufficient to distinguish about \SI{87.83}{\%} of solver pairs.

% MI: Enough justification, commented out the following
%For the SAT Competition~2022 Anniversary track instances, this ranking approach is capable of correctly deciding for almost all pairs of solvers which one is faster (about \SI{97.45}{\%}; $\sigma = \SI{3.68}{\%}$).
%Moreover, it decides all solvers correctly if the difference in the cluster-label-ranking scores between two solvers is at least \SI{0.11}{}.

This shows that monotonicity and score distances between ranks align quite well between both ranking methods.
The correlation between $s_k$-induced and PAR-2 induced ranks, the discriminatory power of $\gamma_k$, as well as that it enables us to use classification, show the clear advantage of runtime discretization.

%\begin{table}[tbp]
%  \centering
%   \caption{
%     PAR-2 ranking vs. discretized-runtime ranking for the SAT Competition~2022 Anniversary Track instances~\cite{sat2022} ($k = 3$; hierarchical clustering).
%	  Differences are highlighted with a gray background.
%	}
%  \label{tab:discreteranking}
%  \begin{tabular}{
%    c
%    >{\centering\arraybackslash}p{1.8cm}p{3.6cm}
%    >{\centering\arraybackslash}p{1.8cm}p{3.6cm}
%  }
%    \hline
%    Rank & \multicolumn{2}{c}{PAR-2 Ranking} & \multicolumn{2}{c}{Cluster-Label Ranking} \\
%    \hline
%1   & 2806.400 & Kissat\_MAB\_ESA           & 1.172 & Kissat\_MAB\_ESA \\
%2   & 2810.959 & kissat-sc2022-bulky        & 1.183 & kissat-sc2022-bulky \\
%3   & 2830.143 & ekissat-mab-gb-db          & 1.185 & ekissat-mab-gb-db \\
%\cellcolor{gray!15} 4   & \cellcolor{gray!15} 2833.081 & \cellcolor{gray!15} Kissat\_MAB\_UCB           & \cellcolor{gray!15} 1.186 & \cellcolor{gray!15} kissat\_inc \\
%\cellcolor{gray!15} 5   & \cellcolor{gray!15} 2833.397 & \cellcolor{gray!15} kissat\_inc                & \cellcolor{gray!15} 1.186 & \cellcolor{gray!15} Kissat\_MAB\_UCB \\
%6   & 2841.734 & ekissat-mab-db-v1          & 1.192 & ekissat-mab-db-v1 \\
%7   & 2842.666 & Kissat\_MAB\_MOSS          & 1.192 & Kissat\_MAB\_MOSS \\
%8   & 2857.219 & Kissat\_MAB-HyWalk         & 1.195 & Kissat\_MAB-HyWalk \\
%9   & 2866.960 & kissat-sc2022-light        & 1.120 & kissat-sc2022-light \\
%10  & 2896.371 & kissat-els-v2              & 1.216 & kissat-els-v2 \\
%11  & 2950.179 & hKis-unsat                 & 1.229 & hKis-unsat \\
%12  & 2964.729 & Kissat\_adaptive\_restart  & 1.234 & Kissat\_adaptive\_restart \\
%13  & 2974.990 & SeqFROST-NoExtend          & 1.247 & SeqFROST-NoExtend \\
%14  & 3010.642 & Cadical\_ESA               & 1.248 & Cadical\_ESA \\
%\cellcolor{gray!15} 15  & \cellcolor{gray!15} 3012.152 & \cellcolor{gray!15} kissat-els-v1              & \cellcolor{gray!15} 1.258 & \cellcolor{gray!15} CadicalReorder \\
%\cellcolor{gray!15} 16  & \cellcolor{gray!15} 3028.922 & \cellcolor{gray!15} CadicalReorder             & \cellcolor{gray!15} 1.264 & \cellcolor{gray!15} kissat-els-v1 \\
%\cellcolor{gray!15} 17  & \cellcolor{gray!15} 3042.540 & \cellcolor{gray!15} cadical\_rel\_Scavel       & \cellcolor{gray!15} 1.278 & \cellcolor{gray!15} CaDiCaL\_DVDL\_V1 \\
%\cellcolor{gray!15} 18  & \cellcolor{gray!15} 3082.293 & \cellcolor{gray!15} kissat\_relaxed            & \cellcolor{gray!15} 1.283 & \cellcolor{gray!15} CaDiCaL\_DVDL\_V2 \\
%\cellcolor{gray!15} 19  & \cellcolor{gray!15} 3089.304 & \cellcolor{gray!15} CaDiCaL\_DVDL\_V1          & \cellcolor{gray!15} 1.298 & \cellcolor{gray!15} kissat\_relaxed \\
%\cellcolor{gray!15} 20  & \cellcolor{gray!15} 3095.118 & \cellcolor{gray!15} CaDiCaL\_DVDL\_V2          & \cellcolor{gray!15} 1.361 & \cellcolor{gray!15} cadical\_rel\_Scavel \\
%21  & 3272.674 & glucose-reboot             & 1.379 & glucose-reboot \\
%\cellcolor{gray!15} 22  & \cellcolor{gray!15} 3287.322 & \cellcolor{gray!15} LStech-Maple-HyWalk        & \cellcolor{gray!15} 1.414 & \cellcolor{gray!15} hCaD\_V1-psids \\
%23  & 3288.798 & LSTech\_Maple              & 1.446 & LSTech\_Maple \\
%\cellcolor{gray!15} 24  & \cellcolor{gray!15} 3399.281 & \cellcolor{gray!15} SLIME-SC-2022-beta         & \cellcolor{gray!15} 1.469 & \cellcolor{gray!15} LStech-Maple-HyWalk \\
%\cellcolor{gray!15} 25  & \cellcolor{gray!15} 3410.345 & \cellcolor{gray!15} SLIME-SC-2022              & \cellcolor{gray!15} 1.470 & \cellcolor{gray!15} SLIME-SC-2022-beta \\
%\cellcolor{gray!15} 26  & \cellcolor{gray!15} 3430.403 & \cellcolor{gray!15} hCaD\_V1-psids             & \cellcolor{gray!15} 1.523 & \cellcolor{gray!15} SLIME-SC-2022 \\
%27  & 3504.244 & MapleLCMDistChrBt-DL-v3    & 1.542 & MapleLCMDistChrBt-DL-v3 \\
%28  & 4750.808 & IsaSAT                     & 2.062 & IsaSAT \\
%    \hline
%  \end{tabular}
%\end{table}


\subsection{Instance Sampling}
\label{sec:main-sampling}

Selecting an instance based on the model is the core functionality of our method (cf. Algorithm~\ref{algALBenchmark}, Line 2). 
In this section, we introduce our sampling strategies that make use of the model's label-prediction function $f$ which are inspired by existing work within the realms of active learning~\cite{settles2009active}.
We implement a model-uncertainty based approach and a model-based information-gain sampling strategy.
These methods require the model's predictions to also include probabilities for the $k$ possible runtime labels denoted by $f' : \mathcal{\hat A} \times \mathcal{I} \rightarrow \left[0, 1\right]^k$. 


\subsubsection{Model Uncertainty Based Sampling.}

The model-uncertainty-based approach simply selects the instance that is closest to the model's decision boundary.

\begin{equation*}
  \underset{e \in \mathcal{I} \setminus \tilde{\mathcal{I}}}{\arg\min} \left\lvert \frac{1}{k} - \max_{n \in \left\lbrace 1, \dots, k \right\rbrace} f'\!\left(\hat{a}, e\right)_{n} \right\rvert
\end{equation*}


\subsubsection{Information Gain Based Sampling.}

The model-based information-gain sampling strategy selects the instance with the highest expected entropy reduction.
To be more specific, we select the instance $e$ that maximizes $IG(e)$ which is specified as follows. 

\begin{equation*}
  \operatorname{IG}(e) := \operatorname{H}(e) - \sum_{n = 1}^{k} f'(\hat{a}, e)_{n} \operatorname{H}(e, n)
\end{equation*}

Here $\operatorname{H}(e)$ denotes the entropy of the runtime labels $\gamma(a, e)$ for all $a \in \mathcal{A}$ and $\operatorname{H}(e, n)$ denotes the entropy of the runtime labels including the runtime label of $\hat{a}$ which is $n$.
The term $\operatorname{H}(e, n)$ is computed for every possible runtime label $n \in \{1, \dots, k\}$.

%JB: if this is info gain regarding runtimes, doesn't Eq. (4) mean that we would prefer instances with low entropy of runtime distribution after considering the new solver, i.e., instances that are not very discriminatory?
%TF: I don't think so. We want to maximize the reduction in entropy of the runtime labels. Low entropy afterward -> we know that the solver behaves similarly to existing ones.



%Finally, the voting-based committee-disagreement approach requires a committee of $c$ models.
%Rather than having a single prediction function $f$, there is a separate function $f_j$ for all committee members $j \in \left\lbrace 1, \dots, c  \right\rbrace$.
%We then select the instance $i$ whose entropy of runtime-label votes by the different ensemble members is the highest.



% MI: The following belongs either to Related Work or to Experimental Design (commented out)
%\subsubsection{Baselines and existing approaches.}
%\label{sec:sampling1}
%Apart from obvious baseline instance-selection strategies, i.e., random sampling and neighborhood-aware sampling (randomly choosing among the instances with the highest amount of non-sampled $k$ neighbors), we also implement existing instance-selection strategies.

%Bossek and Wagner~\cite{Bossek021a} introduce a custom ranking-score metric incorporating the number of \emph{good} pairs and \emph{bad} pairs.
%JB: some selection methods, like this one, are not mentioned later anymore (in particular, not in experimental design), so we don't need to describe them in detail (maybe mention them shortly if used in preliminary experiments, but turned out to be not useful)
%TF: I've cut the section down a bit
%A good solver pair, regarding a particular instance, are solvers that perform according to their global ranking.
%Otherwise, it is a \emph{bad} pair.
%Based on this classification, they assign a custom score to each instance.
%By selecting instances with the highest score, they try to favor instances that are predictive of the final ranking.

%Gent~et~al.~\cite{GentHJKMNN14} make use of a concept known from racing for automated model selection.
%Given a particular instance and a parameter $\rho$, a solver is $\rho$-dominated if it is at least $\rho$ times worse than the best solver on this problem instance.
%By selecting instances with the highest amount of $\rho$-dominated solvers, they hope to sample instances that provide the best discriminatory power to distinguish the runtimes of a new contestant.

%Finally, we also implement some of the selection approaches described by Matricon~et.al.~\cite{MatriconAFSH21}.
%Besides uniformly random sampling and the aforementioned discrimination-based method by Gent~et~al.~\cite{GentHJKMNN14}, they also describe a variance-based selection approach.
%The variance-based strategy selects instances with the highest runtime variance per average runtime.
%We also implement a logarithmized version of this, i.e., the highest log-runtime variance per average log runtime.


\subsection{Stopping Criteria}
\label{sec:main-stopping}

%Similar to the instance-selection strategies, we can also divide stopping criteria into baseline and active-learning-based approaches.
%Also, refer to~Algorithm~\ref{algALBenchmark}, line~3.
%While a fixed subset-size criterion, e.g., only sample \SI{20}{\%} of all available instances, is an example of the former, a stopping criterion based on a Wilcoxon signed-rank test or the convergence of the model's predictions is an example for the latter.

In this section, we present two dynamic stopping criteria, the Wilcoxon and the Model Uncertainty stopping criteria (cf. Algorithm~\ref{algALBenchmark}, Line~1). 


\subsubsection{Wilcoxon Stopping Criterion}

The Wilcoxon stopping criterion stops the iterative active learning algorithm when we are on average certain enough that the predicted runtime labels are sufficiently different in comparison to the other solvers.

To be more specific, we use the average $p$-value $W_{\hat{a}}$ of a Wilcoxon signed-rank test $w(S,P)$ of the two runtime label distributions $S=\{ \gamma(a, e) \mid e \in \mathcal{I} \}$ and $P=\{ f(\hat a, e) \mid e \in \mathcal{I} \}$. 

\begin{equation*}
  W_{\hat{a}} := \frac{1}{\lvert \mathcal{A} \rvert} \sum_{a \in \mathcal{A}} \operatorname{w}(S, P)
\end{equation*}

To improve the stability of this criterion, we use an exponential moving average to smooth out outliers as follows and stop as soon as $W^{(i)}_{\hat{a}}$ drops below a certain threshold.

\begin{align*}
	W_{\exp}^{\left(0\right)} &:= 1\\
	W_{\exp}^{\left(i\right)} &:= \beta W_{\hat{a}} + \left(1 - \beta\right) W_{\exp}^{\left(i - 1\right)}
\end{align*}


\subsubsection{Model Convergence Stopping Criterion}

The model-convergence stopping criterion is less sophisticated in comparison.
It simply stops the active-learning process if within the last $l$ iterations the ranking that is induced by the model's predictions (Equation~\ref{eq:rankingeq}) remains unchanged.
Note that the concrete values of $s_k(\hat a)$ might still change.
We are solemnly interested in the induced ranking in this case.


\section{Experimental Design}
\label{sec:exdesign}

Given all the previously presented instantiations for Algorithm~\ref{algALBenchmark}, this section briefly outlines our experimental design, including our optimization goal, evaluation framework, used data sets, hyper-parameter choices, as well as implementation details.
% hyper-parameter values that we experiment with

\subsection{Optimization Goal}
\label{sec:goal}

As already stated in the introductory section, this work addresses the \emph{New-Solver Problem} (cf.~Definition~\ref{def:new-solver-problem}).
By running Algorithm~\ref{algALBenchmark}, we obtain a prediction model $\mathcal{M}$ that provides us with a scoring function \emph{score}.
Also, it outputs the fraction of runtime that is needed to arrive at its conclusion.
First and foremost, our goal is to provide the engineer of new SAT solvers with an accurate ranking.
We define the \emph{ranking accuracy} $O_{\operatorname{acc}} \in \left[0, 1\right]$ (higher is better) by the fraction of pairs $\left(\hat{a}, a\right)$ for all $a \in \mathcal{A}$ that are decided correctly by the given ranking (cf. Algorithm~\ref{alg:eval}).
For now, we exclude the equality of solvers since data shows that the PAR-2 scores of all our solvers are different.
So, possible ranking decisions may be, for example, solver $a$ is better than $b$ or $b$ is better than $a$.
The ranking accuracy is affected by whether this decision is correctly made.
Matricon et. al.~\cite{MatriconAFSH21} also evaluate their approach by the fraction of solver pairs that is correctly decided.
Second, we also have to optimize for runtime.
The \emph{fraction of runtime} that the algorithm needs to arrive at its conclusion is denoted by $O_{\operatorname{rt}} \in \left[0, 1\right]$ (lower is better).
This metric puts the runtime summed over the sampled instances in relation to the runtime summed over all instances in the dataset (cf. Algorithm~\ref{alg:eval}).
Overall, we want to find an approach that maximizes
%
\begin{equation}
  O_\delta := \delta O_{\operatorname{acc}} + \left(1 - \delta\right) \left(1 - O_{\operatorname{rt}}\right) \enspace \textrm{,}
  \label{eq:opt}
\end{equation} 
%
whereby $\delta \in \left[0, 1\right]$ allows for linear weighting between the two optimization goals $O_{\operatorname{acc}}$ and $O_{\operatorname{rt}}$.
Plotting the approaches that maximize $O_\delta$ for all $\delta \in \left[0, 1\right]$ on a $O_{\operatorname{rt}}$-$O_{\operatorname{acc}}$-diagram provides us with a Pareto front of the best approaches for different optimization-goal weightings.

\subsection{Evaluation Framework}
\label{sec:evalframe}

To evaluate a concrete instantiation of Algorithm~\ref{algALBenchmark} (a concrete choice for all the sub-routines), we perform cross-validation on our set of solvers.
That means that each solver once plays the role of the new solver.
Algorithm~\ref{alg:eval} shows this.
Note that the \emph{new} solver in each iteration is excluded from the set of solvers $\mathcal{A}$ to avoid data leakage.
Also, the runtime function $r$ is restricted to $\mathcal{A} \times \mathcal{I}$.

\begin{algorithm}[htbp]
  \caption{Evaluation Framework}
  \label{alg:eval}

  \KwIn{A pool of instances $\mathcal{I}$, a set of solvers $\mathcal{S}$ with known runtimes $r\!: \mathcal{S} \times \mathcal{I} \rightarrow \left[0, \tau\right] \cup \left\lbrace \perp \right\rbrace$, and PAR-2 scores $par2\!: \mathcal{S} \rightarrow \mathbb{R}$ for each solver.}

  $res \leftarrow \emptyset$

  \For{$\hat{a} \in \mathcal{S}$}{
    $\mathcal{A} \leftarrow \mathcal{S} \setminus \left\lbrace \hat{a} \right\rbrace$

    $\left( score,\, \tilde{\mathcal{I}} \right) \leftarrow \operatorname{runALAlgorithm}\!\left(\mathcal{I}, \mathcal{A}, \left.r\right|_{\mathcal{A} \times \mathcal{I}}, \hat{a}\right)$ \tcp*{Refer to Algorithm~\ref{algALBenchmark}}

    $O_{\operatorname{acc}} \leftarrow \left(\sum_{a \in \mathcal{A}} \left[\operatorname{sign}\!\left(score\!\left(a\right) - score\!\left(\hat{a}\right)\right) = \operatorname{sign}\!\left(par2\!\left(a\right) - par2\!\left(\hat{a}\right)\right) \right]\right) / |\mathcal{A}|$
    
    \tcp*{Iverson-bracket notation}
    %JB: maybe we can simplify the expresson, since readers might be unfamiliar with that notation (I had to look it up)

    $O_{\operatorname{rt}} \leftarrow \left(\sum_{i \in \tilde{\mathcal{I}}} r\!\left(i, \hat{a}\right)\right) / \left(\sum_{i \in \mathcal{I}} r\!\left(i, \hat{a}\right)\right)$

    $res \leftarrow res \cup \left\lbrace \left( O_{\operatorname{acc}},\, O_{\operatorname{rt}} \right) \right\rbrace$
  }

  $\left( \bar{O}_{\operatorname{acc}},\, \bar{O}_{\operatorname{rt}} \right) \leftarrow \operatorname{mean}\!\left( res \right)$

  \KwOut{Return the average ranking accuracy $\bar{O}_{\operatorname{acc}}$ and the average fraction of runtime needed $\bar{O}_{\operatorname{rt}}$.}
\end{algorithm}

\subsection{Data}

In our experiments, we work with the SAT~Competition~2022 Anniversary Track instances~\cite{sat2022}.
The dataset consists of 5355 instances with respective runtime data.
It has complete runtimes of 28 solvers.
We use the GBD metadata database~\cite{IserS18} to provide us with the aforementioned instance features, problem instances, and solver runtimes.
Thereby, all our approaches make use of the 56 base features that are maintained in the \emph{base} database of GBD~\cite{IserS18}.
They are inspired by the SATzilla features~\cite{features}.
Those features comprise general instance-size features and graph-representation features among others.
All features are numeric and fortunately free of any missing values.
We drop 10 out of 56 features because of zero variance.
For hyper-parameter tuning, we randomly sample \SI{10}{\%} of the complete set of 5355 instances with stratification regarding the instance's family.
All instance families that are too \emph{small}, i.e., \SI{10}{\%} of them corresponds to less than one instance, are put into one meta-family for stratification.
This smaller dataset allows for a more extensive exploration of hyper-parameter space.

\subsection{Hyper-parameters}
\label{sec:hyper}

Given Algorithm~\ref{algALBenchmark}, there are several possible instantiations for the three phases, i.e., \emph{selection}, \emph{stopping}, and \emph{ranking}.
Also, there are different choices for the runtime-label prediction model.
Note that we are not considering all previously listed approaches since a grid search of all combinations would be infeasible.
Rather, we filter approaches based on preliminary experimental results (cf. Section~\ref{sec:evalprel}) and do the main end-to-end experiments only with a subset.
The end-to-end experiment configurations are given below.

\subsubsection{Ranking.}

Regarding \emph{ranking} (cf. Section~\ref{sec:main-model}), we experiment with the following approaches, including our used hyper-parameter values:

\begin{itemize}
  \item Observed PAR-2 ranking of already sampled instances
  \item Predicted ranking induced by runtime-label predictions
  \begin{itemize}
    \item
    History size: consider the latest 1, 10, 20, 30, or 40 predictions within a voting approach for stability.
    The latest $x$ predictions vote on the winner.
    %JB: confuses me a bit as it sounds like we rank first and then vote on a winner; isn't it the other way around?
    %TF: Removed confusing sentence
    \item
    Fallback threshold: if the difference of scores between two solvers drops below \SI{0.01}, \SI{0.05}, or \SI{0.1}, use the partially observed PAR-2 ranking as a tie-breaker.
    %JB: as clarification: one these two solvers has to be the new solver, right? if yes, would write it explicitly
    %TF: Not necessarily
    As discussed in Section~\ref{sec:main-model}, ranking decisions are all correct if the difference in scores between two solvers exceeds a certain value.
  \end{itemize}
\end{itemize}

\subsubsection{Selection.}

For \emph{selection} (cf. Section~\ref{sec:main-sampling}), we experiment with the following methods, including our used hyper-parameter values:

\begin{itemize}
  \item Random sampling 
  \item Model-based uncertainty sampling
  \begin{itemize}
    \item Fallback threshold: use random sampling for the first \SI{0}{\%}, \SI{5}{\%}, \SI{10}{\%}, \SI{15}{\%}, or \SI{20}{\%} of instances to explore the instance space.
    \item Runtime scaling: prefer instances with the greatest uncertainty per average (over all solvers) runtime (\texttt{True} or \texttt{False}).
  \end{itemize}

  \item Model-based information-gain sampling
  \begin{itemize}
    \item Fallback threshold: use random sampling for the first \SI{0}{\%}, \SI{5}{\%}, \SI{10}{\%}, \SI{15}{\%}, or \SI{20}{\%} of instances to explore the instance space.
    \item Runtime scaling: prefer instances with the greatest information-gain per average runtime (\texttt{True} or \texttt{False}).
  \end{itemize}

  \item Neighborhood-aware random sampling (preliminary experiments only); randomly chooses among the instances with the highest amount of non-sampled $k$ neighbors
  \item Ranking-based sampling~\cite{Bossek021a} (preliminary experiments only)
  \item Discrimination-based sampling~\cite{GentHJKMNN14} (preliminary experiments only)
  \item Variance-based sampling~\cite{MatriconAFSH21} (preliminary experiments only)
\end{itemize}

\subsubsection{Stopping.}

For \emph{stopping} decisions (cf. Section~\ref{sec:main-stopping}), we experiment with the following criteria, including our used hyper-parameter values:

\begin{itemize}
  \item Fixed subset size of \SI{10}{\%} or \SI{20}{\%} of instances
  \item Ranking convergence criterion
  \begin{itemize}
    \item Minimum amount: sample at least \SI{2}{\%}, \SI{8}{\%}, \SI{10}{\%}, or \SI{12}{\%} of instances before applying the criterion.
    \item Convergence duration: stop if the predicted ranking stays the same for the last \SI{1}{\%} or \SI{2}{\%} of sampled instances.
    %JB: why this fraction and not an absolute number of instances? the definition here might make stopping in early iterations rather likely, as just very few consecutive instances are needed (which might happen by coincidence; e.g., for ~5k instances, 2% minimum amount-> ~100 instances, 1% convergence duration -> ~1 instance), while the criterion becomes harder in later iterations
    %TF: for that reason, it is combined with the "minimum amount". Using fractions makes the approach more useful when adapting it to benchmarks of different sizes.
    %JB: yes, but (1) even combination with "minimum amount" can make stopping very easy (see my example) and (2) fraction for convergence could also be relative to absolute number of instances (which is constant over AL run) rather than fraction of currently sampled instances (which increases over AL run)
    %TF: I'm sure that both would make sense. Isn't 1% convergence duration of 5k about ~50 instances?
  \end{itemize}

  \item Wilcoxon criterion
  \begin{itemize}
    \item Minimum amount: sample at least \SI{2}{\%}, \SI{8}{\%}, \SI{10}{\%}, or \SI{12}{\%} of instances before applying the criterion.
    \item Average of $p$-values to drop below: \SI{5}{\%}.
    \item Exponential-moving average: incorporate previous significance values by using an EMA with $\beta = 0.1$ or $\beta = 0.7$.
  \end{itemize}
\end{itemize}

\subsubsection{Runtime-label prediction model.}

As runtime-label prediction model, we only make use of one fixed model since an exhaustive grid search would be infeasible (cf.~Section~\ref{sec:evalprel}).
Our model of choice is an ensemble, stacking a quadratic-discriminant analysis\footnote{With a regularization parameter of zero and a singular value threshold of zero} onto a random forest model\footnote{With \emph{entropy} splitting criterion and \emph{balanced} class weights} using a decision tree\footnote{With \emph{Gini} impurity splitting criterion; choosing the best split for each branching decision and maximum tree depth of 5 levels} to weight its members.
If not stated explicitly, all other model parameters default to the presets of \emph{scikit-learn}\footnote{\url{https://scikit-learn.org/stable/index.html}; Version 1.0.2}~\cite{scikit-learn}.

%the proposed runtime label ranking, (2) the PAR-2 subsample ranking, and (3) the runtime label ranking that uses the PAR-2 subsample ranking as a fallback when performance differences drop below a threshold.
%Possible values are \SI{0.01}{}, \SI{0.05}{}, and \SI{0.1}{}.
%Also, we experiment with different lengths of the prediction history that is used for stabilizing the ranking.
%Possible values are 10, 20, 30, 40, and 50.

\subsection{Implementation Details}

For reproducibility, our source code, all experiments, and data are available on GitHub\footnote{\url{https://github.com/mathefuchs/al-for-SAT solver-benchmarking}}.
Our code is implemented in \textsc{Python} using \emph{scikit-learn}~\cite{scikit-learn} for making predictions and \emph{gbd-tools}~\cite{IserS18} for SAT-instance retrieval.


\section{Evaluation}
\label{sec:eval}
Before evaluating our approach end-to-end as described in Section~\ref{sec:exdesign}, we discuss the choice of the runtime-discretization method and the machine-learning predictor.
Thereafter, we look at the results of our active-learning approach.
Selection decisions of the aforementioned approach also reveal the importance of different problem-instance families to our model.
Instance families comprise instances that are derived from the same application domain, e.g., planning, cryptography, etc., and are a useful tool for both analyzing solver performance and portfolio creation.
%JB: instance families not really motivated till now, only appear in abstract and in text regarding stratification
%TF: Added a sentence for clarification. We could introduce more details in section 4.3.

\subsection{Runtime Prediction}
\label{sec:evalprel}
%JB: if we dedicate a whole section to these results, we should probably rename this section (not "preliminary" in the typical sense); for TACAS, I guess this can be cut a bit
%TF: cut the section down a bit, renamed to "runtime prediction"
An exhaustive grid search of all active-learning approaches outlined in Section~\ref{sec:main} and different runtime-prediction models is infeasible.
Therefore, we use the runtime-prediction model that produced the best results for all further experiments.
We compare runtime predictors by looking at the \emph{Matthews Correlation Coefficient} (MCC) scores~\cite{gorodkin2004comparing,matthews1975comparison} of the classification task.
MCC scores are great in dealing with class imbalances in contrast to conventional metrics like accuracy.

% \begin{table}
%   \centering
%   \caption{
%   	Time-out prediction Matthews Correlation Coefficient performances.
%   	Results are obtained using cross-validation.
%   	We provide \SI{64}{\%} of ground-truth for training, use \SI{16}{\%} as a validation set, and \SI{20}{\%} as a test set (5-fold cross-validation).
%   }
%   \label{tab:mcc}
%   \begin{tabular}{lc}
%     Prediction Model & Avg. MCC (Std.) \\
%     \hline
%     Stacking ensemble (QDA, RF)           & 0.9527 (0.0292) \\
%     Quadratic-discriminant analysis (QDA) & 0.9290 (0.0339) \\
%     Random forest (RF)                    & 0.8530 (0.0479) \\
%     AdaBoost with decision trees          & 0.8384 (0.0444) \\
%     AdaBoost with logistic regression     & 0.8158 (0.0985) \\
%     Decision tree                         & 0.8059 (0.0707) \\
%     Logistic regression                   & 0.8052 (0.1018) \\
%     kNN classifier                        & 0.7885 (0.1521) \\
%     MLP classifier                        & 0.7760 (0.1408) \\
%     Support-vector classifier             & 0.7757 (0.2149) \\
%     Naive Bayes                           & 0.7306 (0.1394) \\
%     \hline
%   \end{tabular}
% \end{table}

% Stacking MCC
% 0.888221577313471
% 0.04753979056930393

%To obtain the results in Table~\ref{tab:mcc},
We perform cross-validation among all solvers: we assume that all other solver runtimes are known and provide \SI{64}{\%} of ground-truth runtime labels of the target solver for training, use \SI{16}{\%} as a validation set for hyper-parameter tuning of the runtime predictor, and \SI{20}{\%} as a test set (5-fold cross-validation).
This is repeated for each solver once.
Note that we perform cross-validation on two levels, i.e., among the solvers and the target solver runtimes.
We report the average performances on the test sets.

ML classification models that we have tried include random forests, decision trees, quadratic-discriminant analysis, AdaBoost, logistic regression, $k$-nearest neighbor, multi-layer perceptrons, support vector machines, and naive Bayes.
A stacking ensemble consisting of quadratic-discriminant analysis and random forests performed best.
It produces an MCC score of \SI{0.9527}{} for time-out prediction and \SI{0.8882}{} for runtime-label prediction.
The results are significantly better than any other model that we have tried regarding a Wilcoxon signed-rank test with $\alpha = \SI{5}{\%}$.

%Since the differences in the MCC-performance distributions of the best and second-best approaches are statistically significant regarding a Wilcoxon signed-rank test with $\alpha = \SI{5}{\%}$, we only use the best approach for all further experiments as a prediction model.

For runtime discretization, we use hierarchical clustering with $k = 3$ using a log-single-link criterion for all further experiments.
Each non-time-out runtime starts in a separate interval.
We then gradually merge intervals whose single-link logarithmic distance is the smallest until the desired number of partitions is reached.
As already mentioned in Section~\ref{sec:main-model}, this approach has good properties regarding the predictiveness of the PAR-2 score ranking and its discriminatory power.
Also, experiments showed that our stacking ensemble performed particularly well for this kind of discretization method.
Other clustering approaches that we have tried include hierarchical clustering with mean-, median- and complete-link criteria, as well as $k$-means and spectral clustering.
%Predicting runtime labels in a similar experiment as above produced an MCC score of on average \SI{88.82}{\%} ($\sigma = \SI{4.75}{\%}$; in comparison to \SI{95.27}{\%} for predicting time-outs).
%JB: "similar experiment as above" - reference unclear; what do you want to say?
%TF: Removed the sentence

\subsection{Experimental Results}
\subsubsection{Tuning our algorithm.}
Our main end-to-end experiments follow the evaluation framework that has been introduced in Section~\ref{sec:evalframe}.
Fig.~\ref{fig:e2eallsolvers} shows the performance of the approaches listed in Section~\ref{sec:hyper} on $O_{\operatorname{rt}}$-$O_{\operatorname{acc}}$-diagrams for the smaller hyper-parameter-tuning dataset.
The plotted lines represent the Pareto front for each possible choice of ranking approaches (Fig.~\ref{fig:annitraincolorranking}), selection approaches (Fig.~\ref{fig:annitraincolorselection}), and stopping criteria (Fig.~\ref{fig:annitraincolorstopping}).
%JB: as far as I understand it, the lines only compare some top-level hyperparameters, but for each of these hyperparemeters, take the best (= Pareto front) out of some more fine-grained settings (the hyperparameters of the hyperparameters, so to speak), so it's no really "each possible choice" in the sense of actual, individual configurations, but rather a combination of configurations
Colors indicate the different hyper-parameter instantiations.
To be more specific, they represent the Pareto front of all configurations with the respective instantiations.

Regarding the ranking approach (Fig.~\ref{fig:annitraincolorranking}), using the ranking that is induced by our runtime-label predictions consistently performs better than the partially observed PAR-2 ranking for each possible value of $\delta$.
%JB: as in previous comment: consistent outperformance not guaranteed in case the lines are not individual approaches, but Pareto fronts within the compared categories (which means that different parts of the line might be related to different hyperparameter settings)
%This was expected since a possible bias in the selection decisions of the model is accounted for when using the model's predictions for ranking.
This is expected since selection decisions are not random.
For example, we might sample more instances of one family if it benefits discrimination of solvers.
While the partially observed PAR-2 score is skewed, the prediction model can account for this.

Regarding the selection approaches (Fig.~\ref{fig:annitraincolorselection}), the model-based uncertainty strategy performs best in most cases.
However, the model-based information-gain sampling is beneficial if runtime is strongly favored (very small $\delta$; runtime fraction less than \SI{5}{\%}).
%JB: we should also describe performance of random sampling, which is visible in plot as well

Regarding the stopping criterion (Fig.~\ref{fig:annitraincolorstopping}), the ranking convergence criterion performs the most consistently well.
If PAR-2 accuracy is strongly favored (very high $\delta$), the Wilcoxon stopping criterion performs better.

\begin{figure}[tbp!]
  \centering
  \begin{subfigure}{1.0\textwidth}
    \resizebox{\textwidth}{!}{
      \graphicspath{{../plots/}}
      \input{../plots/anni_train_color_ranking.pgf}
    }
    \caption{
    	$O_{\operatorname{rt}}$-$O_{\operatorname{acc}}$-diagram showing Pareto fronts for configurations with the given ranking approaches.
    	The lines show the best performances of approaches using runtime-label-prediction ranking and partially observed PAR-2 ranking respectively.
    }
    \label{fig:annitraincolorranking}
  \end{subfigure}
  \\
  \vspace{0.2cm}
  \begin{subfigure}{1.0\textwidth}
    \resizebox{\textwidth}{!}{
      \graphicspath{{../plots/}}
      \input{../plots/anni_train_color_selection.pgf}
    }
    \caption{
    	$O_{\operatorname{rt}}$-$O_{\operatorname{acc}}$-diagram showing Pareto fronts for configurations with the given instance-selection approaches.
    	The lines show the best performances of approaches using uncertainty-based, information-gain-based, and random instance selection respectively.
    }
    \label{fig:annitraincolorselection}
  \end{subfigure}
  \\
  \vspace{0.2cm}
  \begin{subfigure}{1.0\textwidth}
    \resizebox{\textwidth}{!}{
      \graphicspath{{../plots/}}
      \input{../plots/anni_train_color_stopping.pgf}
    }
    \caption{
    	$O_{\operatorname{rt}}$-$O_{\operatorname{acc}}$-diagram showing Pareto fronts for configurations with the given stopping criteria.
    	The lines show the best performances of approaches using the ranking-convergence criterion, Wilcoxon criterion, and a fixed subset size respectively.
    }
    \label{fig:annitraincolorstopping}
  \end{subfigure}

  \caption{
  	$O_{\operatorname{rt}}$-$O_{\operatorname{acc}}$-diagrams showing the performance of different hyper-parameter instantiations.
  	The x-axis shows the fraction of runtime in comparison to the time needed to evaluate a solver on all instances.
  	The y-axis shows the accuracy of the predicted rank in comparison to the true PAR-2 rank.
  	Each line entails the front of Pareto-optimal configurations with the respective instantiation.
  }
  %JB: not sure if the color palette in the plots is grayscale-/color-blind-friendly; consider changing the palette (see https://colorbrewer2.org/#type=qualitative&scheme=Set3&n=3 ) and using different line types for each color
  \label{fig:e2eallsolvers}
\end{figure}

% One of the main observations is that our model-uncertainty-based sampling method (blue points) consistently outperformed random selection (red points) for almost any value of $\delta$ within Eq.~\ref{eq:opt}.
% Moreover, our ranking-convergence criterion (dot-shaped points) is particularly suited to achieve a good ranking accuracy fast.
% In comparison, our Wilcoxon-based criterion is suited better for high-accuracy predictions.
% This comes at a high runtime cost though.
% A good trade-off is achieved for $\delta = 0.85$ with an average PAR-2 ranking performance of \SI{91.83}{\%} and a runtime requirement of on average \SI{23.05}{\%} of the time it would take to evaluate the complete dataset.

% \begin{table}[tb!]
%   %JB: really hard to extract take-aways by looking at this table (several hyperprameter settings and several metrics, the hyperparameter settings of our approach change from row to row)
%   \centering
%   \caption{
%   	Pareto-optimal approaches for different choices of $\delta$ regarding our smaller hyper-parameter tuning dataset.
%   	Rows with no changes in comparison to their predecessor are hidden.
%   	%JB: no change in what? all columns except delta?
%   	Ranking approaches are either predicted cluster-label ranking (Pred.) or partially observed PAR-2 ranking (PAR-2).
%   	Selection approaches are either uncertainty-based selection (Uncert.) or information-gain-based selection (IG).
%   	Stopping criteria are either ranking convergence (Conv.) or Wilcoxon-based (Wilcoxon).
%   	%JB: How are the values for ranking/selection/stopping determined -- whatever is best for a particular delta?
%   }
%   \label{tab:paretotable}
%   \vspace{0.2cm}
%   \begin{tabular}{
%     >{\centering\arraybackslash}m{0.6cm}
%     >{\centering\arraybackslash}m{1.6cm}
%     >{\centering\arraybackslash}m{1.6cm}
%     >{\centering\arraybackslash}m{1.6cm}
%     >{\centering\arraybackslash}m{1.5cm}
%     >{\centering\arraybackslash}m{1.5cm}
%     >{\centering\arraybackslash}m{1.5cm}
%     >{\centering\arraybackslash}m{1.5cm}
%   }
%     \hline
%     $\delta$ & Ranking & Selection & Stopping & Fraction Runtime ($O_{\operatorname{rt}}$) & Fraction Instances & $O_{\operatorname{acc}}$ & $O_{\delta}$ \\
%     \hline
%     0.00 & PAR-2 & Uncert. & Conv. & 0.021 & 0.025 & 0.593 & 0.979 \\
%     0.05 & Pred. & Uncert. & Conv. & 0.024 & 0.158 & 0.765 & 0.966 \\
%     0.10 & Pred. & Uncert. & Conv. & 0.025 & 0.128 & 0.779 & 0.955 \\
%     0.15 & Pred. & IG & Conv. & 0.028 & 0.112 & 0.802 & 0.946 \\
%     $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
%     0.50 & Pred. & IG & Conv. & 0.028 & 0.112 & 0.802 & 0.887 \\
%     0.55 & Pred. & Uncert. & Conv. & 0.053 & 0.142 & 0.824 & 0.880 \\
%     0.60 & Pred. & Uncert. & Conv. & 0.146 & 0.082 & 0.897 & 0.880 \\
%     $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
%     0.75 & Pred. & Uncert. & Conv. & 0.146 & 0.082 & 0.897 & 0.886 \\
%     0.80 & Pred. & Uncert. & Conv. & 0.219 & 0.128 & 0.917 & 0.890 \\
%     \hline
%     0.85 & Pred. & Uncert. & Conv. & 0.272 & 0.171 & 0.927 & 0.897 \\
%     \hline
%     0.90 & Pred. & Uncert. & Wilcoxon & 0.894 & 0.888 & 0.996 & 0.907 \\
%     0.95 & Pred. & Uncert. & Wilcoxon & 0.931 & 0.924 & 1.000 & 0.953 \\
%     1.00 & Pred. & Uncert. & Wilcoxon & 0.989 & 0.981 & 1.000 & 1.000 \\
%     \hline
%   \end{tabular}
% \end{table}

\begin{figure}[tb!]
  \centering
  \resizebox{0.78\textwidth}{!}{
    \graphicspath{{../plots/}}
    \input{../plots/anni_train_optimization_goal.pgf}
  }
  \caption{
    Scatter plot showing different instantiations of our active-learning strategy for different $\delta$ on our smaller hyper-parameter-tuning dataset.
    The x-axis shows the fraction of runtime $O_{\operatorname{rt}}$ that a particular instantiation needs.
    The y-axis shows the fraction of instances that a particular instantiation selects to arrive at its conclusion.
    The color indicates the weighting between different optimization goals $\delta \in \left[0, 1\right]$.
    A small $\delta$ refers to favoring runtime over accuracy and a big $\delta$ refers to favoring accuracy over runtime.
  }
  \label{fig:annitrainoptgoal}
\end{figure}

Table~\ref{tab:paretotable} also shows the performance of particular hyper-parameter choices for different values of $\delta$.
%JB: some plot for comparison would be better than a truncated table showing a variety hyperparameter configs
Thereby, we observe that for small $\delta$ (runtime more important) our approaches preferably use easy instances to get a good sense of the PAR-2 ranking fast, e.g., row with $\delta = 0.15$.
%JB: the "e.g." part needs more explanation (how can we see that easy instances used? -> fraction runtime vs. fraction instances)
In contrast to that, harder instances are preferred for higher values of $\delta$, i.e., the sampled fraction of runtime is greater than the fraction of instances.
In our opinion, the row with $\delta = 0.85$ represents the best trade-off between runtime and PAR-2 accuracy.
%JB: this is a scientific article, not an opinion piece :-); all deltas represent different trade-offs, so I would not claim to have found a best trade-off (probably depends on user which kind of trade-off is most desired)

\subsubsection{Full dataset evaluation.}
Having selected the most promising hyper-parameters, we run our end-to-end active-learning experiments on the complete Anniversary Track dataset (5355 instances).
The best-performing approach for $\delta = 0.85$ (since this was the most promising trade-off on the tuning set) uses runtime-label prediction ranking, model-based uncertainty sampling, and a ranking-convergence stopping criterion.
Thereby, the sampled instances account for about \SI{5.24}{\%} of instances and about \SI{10.35}{\%} of runtime.
Moreover, the PAR-2 accuracy is about \SI{92.33}{\%}, the average Spearman correlation between the PAR-2 ranking and the predicted ranking scores about \SI{0.9572}{}, and the weighted objective $O_{\delta} = 0.9193$.
%JB: many numbers, but some qualitative (and positively sounding, i.e., saying that our approach indeed is useful) interpretation would be nice

\subsection{Instance-Family Importance}

For all configurations of our approach that sample on average less than \SI{10}{\%} of all 5355 instances, we look at the particular instances and families that are selected by our active-learning approach.
Thereby, the average benchmark set contains about \SI{192.34}{} instances.
Table~\ref{tab:familiesa} lists the ten most frequent families that are chosen.
For example, \emph{planning} instances account for on average \SI{19.14}{} of the \SI{192.34}{} benchmark instances.
In contrast to that, Table~\ref{tab:familiesb} shows the families that are sampled the most often relative to their occurrences in the underlying dataset.
There are, for example, only nine \emph{circuit-multiplier} instances in the complete dataset.
However, we sample on average \SI{1.59}{} of them.
%JB: there should be some (qualitative) interpretation (based on our results, is the sample interesting/special family-wise or not?) rather than just dropping a table into the paper and mainly explaining its structure

% \begin{table}[tb!]
%   %JB: "fraction" here only draws family-specific comparison of sample vs. full dataset, but does not compare overall distribution of families in sample/dataset; might be interesting to have scatterplot with "(relative) frequency of family in sample" vs. "(relative) frequency of family in dataset" (does not allow to show all family labels (though we probably could manually label some families), but allows to show all families)
%   %JB: could also correlate "(relative) frequency of family in sample" with "(relative) frequency of family in dataset"
%   \centering
%   \caption{Top-10 most \emph{important} families regarding the average occurrences and the fraction of selected occurrences regarding our AL approach}
%   \label{tab:families}
%   \begin{subtable}[t]{0.48\textwidth}
%     \centering
%     \caption{Sorted by the average number of occurrences in the sample}
%     \label{tab:familiesa}

%     \begin{tabular}{
%       >{\centering\arraybackslash}m{1.8cm}
%       >{\centering\arraybackslash}m{1.2cm}
%       >{\centering\arraybackslash}m{1.2cm}
%       >{\centering\arraybackslash}m{1.2cm}
%     }
%       \hline
%       Family & In Sample & In Dataset & Fraction (\%) \\
%       \hline
%       planning              & 19.14 &  333 &  5.7 \\
%       subgraph-isomorphism  & 16.27 &  175 &  9.3 \\
%       crypto-graphy         & 13.11 &  311 &  4.2 \\
%       pigeon-hole           &  7.55 &  151 &  5.0 \\
%       fpga-routing          &  6.16 &  73  &  8.4 \\
%       rbsat                 &  5.93 &  113 &  5.2 \\
%       quasigroup-completion &  5.23 &  210 &  2.5 \\
%       unknown               &  4.93 &  169 &  2.9 \\
%       miter                 &  4.84 &  199 &  2.4 \\
%       bitvector             &  4.73 &  214 &  2.2 \\
%       % clique-coloring       &  4.21 &  64  &  0.066 \\
%       % mosoi-289             &  3.75 &  31  &  0.121 \\
%       % hardware-verification &  3.70 &  344 &  0.011 \\
%       % hardware-bmc          &  2.77 &  78  &  0.035 \\
%       % graph-based           &  2.77 &  134 &  0.021 \\
%       % coloring              &  2.66 &  41  &  0.065 \\
%       % waerden               &  2.55 &  43  &  0.059 \\
%       % graph-coloring        &  2.50 &  87  &  0.029 \\
%       % auto-correlation      &  2.39 &  46  &  0.052 \\
%       % knights-problem       &  2.25 &  17  &  0.132 \\
%       %\hline
%       Remaining families    & 104.45 & 3407 & 3.1 \\
%       \hline
%       Sum                   & 192.34 & 5355 & 3.6 \\
%       \hline
%     \end{tabular}
%   \end{subtable}
%   \hfill
%   \begin{subtable}[t]{0.48\textwidth}
%     \centering
%     \caption{Sorted by the fraction of sampled occurrences}
%     \label{tab:familiesb}

%     \begin{tabular}{
%       >{\centering\arraybackslash}m{1.8cm}
%       >{\centering\arraybackslash}m{1.2cm}
%       >{\centering\arraybackslash}m{1.2cm}
%       >{\centering\arraybackslash}m{1.2cm}
%     }
%       \hline
%       Family & In Sample & In Dataset & Fraction (\%) \\
%       \hline
%       circuit-multiplier          & 1.59    &      9      &     17.7 \\
%       subset-cardinality          & 1.55    &      9      &     17.3 \\
%       ssp-0                       & 0.34    &      2      &     17.0 \\
%       sliding-puzzle              & 1.52    &      11     &     13.8 \\
%       knights-problem             & 2.25    &      17     &     13.2 \\
%       genurq                      & 1.55    &      12     &     12.9 \\
%       stone                       & 1.39    &      11     &     12.7 \\
%       mosoi-289                   & 3.75    &      31     &     12.1 \\
%       anti-bandwidth              & 1.52    &      13     &     11.7 \\
%       random                      & 0.91    &      8      &     11.4 \\
%       % spectrum-repacking          & 1.30    &      12     &     0.108631 \\
%       % social-golfer               & 0.95    &      9      &     0.105159 \\
%       % product-configuration       & 0.52    &      5      &     0.103571 \\
%       % minimal-disagreement-parity & 1.54    &      16     &     0.095982 \\
%       % subgraph-isomorphism        & 16.27   &       175   &     0.092959 \\
%       % sgen-balanced               & 0.27    &      3      &     0.089286 \\
%       % pythagorean-triples         & 1.82    &      21     &     0.086735 \\
%       % fpga-routing                & 6.16    &      73     &     0.084393 \\
%       % petrinet-concurrency        & 2.11    &      25     &     0.084286 \\
%       % core-based-generator        & 1.00    &      13     &     0.076923 \\
%       %\hline
%       Remaining families    & 175.97 & 5232 & 3.4 \\
%       \hline
%       Sum                   & 192.34 & 5355 & 3.6 \\
%       \hline
%     \end{tabular}
%   \end{subtable}
% \end{table}

\begin{figure}[tb!]
  \centering
  \resizebox{0.85\textwidth}{!}{
    \graphicspath{{../plots/}}
    \input{../plots/anni_final_families.pgf}
  }
  \caption{
    Scatter plot showing the \emph{importance} of different instance families.
    The x-axis shows the fraction of instances within a particular family in the complete SAT Competition~2022 Anniversary Track benchmark set~\cite{sat2022}.
    The y-axis shows the average fraction of instances within a particular family that is selected by our active-learning strategies.
    The dashed line represents families that are represented with the same fraction in, both, our dataset and the incremental benchmark sets $\tilde{\mathcal{I}}$ that are chosen by our approach.
  }
  \label{fig:annifinalfamilies}
\end{figure}

\section{Conclusion}
In this work, we have discussed possible solutions to the \emph{New-Solver Problem}:
Given a new solver, we want to find its ranking amidst its competitors.
Our approaches provide accurate ranking predictions while only needing a fraction of the runtime resources that a full evaluation on all benchmarking instances would need.
We use a runtime discretization technique as this enables us to transform the regression problem of directly predicting runtimes into the much simpler notion of classification.
We have shown that, albeit being more simple, the classification of discrete runtime labels produces good results.
We have evaluated several ranking algorithms, instance-selection approaches, and stopping criteria within our sequential active-learning process.
A model-uncertainty-based selection approach in combination with runtime-label-prediction ranking and a ranking-convergence stopping criterion showed the consistently best results.
%JB: "consistently best" -- it's the best approach for one delta, but not sure regarding other deltas nor its sensitivity to hyperparameters
We also took a brief look at which instance families are the most prevalent when sampling instances.

\subsection{Future Work}
In future work, it may be of interest to compare further ranking algorithms, instance-selection approaches, and stopping criteria.
Furthermore, it is possible to formulate the runtime discretization as an optimization problem.
Given a dataset, optimization could select the discretization technique with the best discriminatory power rather than selecting one approach upfront by hand.

A major shortcoming is the lack of parallelizability.
Our current approach selects instances one at a time.
Running benchmarks on a computing cluster with $n$ cores benefits from having a batch of $n$ instances at a time.
This is, however, not trivial since, on the one hand, a higher $n$ leads to less \emph{active learning} (because of bigger batch sizes) and, on the other hand, it is not clear how to synchronize the model update and instance selection without a barrier lock, which wastes a lot of runtime resources.

On a more general note, it would make sense to generalize this evaluation framework for arbitrary $\mathcal{NP}$-complete problems.
%JB: is there some methodological generalization needed or can't we just apply the same framework (instantiating the instance-feature part differently) to these use cases?
Those problems share most of the relevant properties of SAT solving, i.e., there are established problem-instance features, a full benchmark run takes days, and creating new problem solvers traditionally requires expert knowledge to hand-select instances of interest.


%
% ---- Bibliography ----
%
\bibliographystyle{splncs04}
\bibliography{literature}

\end{document}
