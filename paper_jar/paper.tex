\documentclass[sn-basic, Numbered]{sn-jnl} % may use "lineno" for line numbers
% template from https://www.springernature.com/gp/authors/campaigns/latex-author-support

\usepackage[ruled,vlined,linesnumbered,norelsize]{algorithm2e}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{siunitx}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage[table]{xcolor}
\usepackage{tikz} % has to be loaded after "xcolor" here, else "option clash"

\newcommand{\cmark}{\ding{51}} % symbols for table comparing benchmarking approaches
\newcommand{\xmark}{\ding{55}}

\DontPrintSemicolon % configuration for algorithm2e
\def\NlSty#1{\textnormal{\fontsize{8}{10}\selectfont{}#1}}
\SetKwSty{texttt}
\SetCommentSty{emph}

\newtheorem{definition}{Definition} % only kind of theorems we use are definitions (well, one)

\raggedbottom

\begin{document}

\title[Active Learning for SAT Solver Benchmarking]{Active Learning for SAT Solver Benchmarking {\large (extended and revised version)}} % short and long title

\author*{\fnm{Tobias} \sur{Fuchs}}\email{tobias.fuchs@kit.edu}
\author{\fnm{Jakob} \sur{Bach}}\email{jakob.bach@kit.edu}
\author{\fnm{Markus} \sur{Iser}}\email{markus.iser@kit.edu}

\affil{\orgname{Karlsruhe Institute of Technology (KIT)}, \orgaddress{\city{Karlsruhe}, \country{Germany}}}

\abstract{
Benchmarking is a crucial phase in the development of algorithms.
This also applies to solvers for the propositional satisfiability (SAT) problem.
Benchmark selection is about choosing representative problem instances that reliably discriminate solvers based on their runtime.
In this paper, we present a dynamic benchmark selection approach based on active learning.
Our approach predicts the rank of a new solver among its competitors with minimum runtime and maximum rank prediction accuracy.
We evaluated this approach on the Anniversary Track dataset from the SAT Competition 2022.
Our selection approach can predict the rank of a new solver after approximately \SI{10}{\%} of the time it would take to run the solver on all instances of this dataset, with a prediction accuracy of approximately \SI{92}{\%}. 
Additionally, we discuss the importance of instance families in the selection process.
In conclusion, our tool offers a reliable method for solver engineers to assess a new solver's performance efficiently.
}

\keywords{Propositional satisfiability, Benchmarking, Active learning}

\maketitle

\section{Introduction}
\label{sec:intro}

One of the main phases of algorithm engineering is benchmarking.
This also applies to propositional satisfiability (SAT), the canonical $\mathcal{NP}$-complete problem.
Benchmarking is, however, quite expensive regarding the runtime of experiments.
While it is still feasible to benchmark a single or small number of SAT solvers, developing new, competitive SAT solvers requires extensive experimentation with a variety of ideas.
The latter often results in a combinatorial explosion of the configuration space of each algorithm~\cite{HutterHL11}.
In particular, a new solver idea is rarely best on the first try.
Thus, it is highly desirable to reduce benchmarking time and discard unpromising ideas early, allowing to test more approaches or spend more time on promising ones.
The field of SAT solver benchmarking is well established, yet traditional benchmark selection approaches do not optimize benchmark runtime.
Instead, they focus on selecting a representative set of instances for ranking solvers~\cite{Gelder11,HoosKSS13}.
In this regard, SAT Competitions typically employ the \mbox{PAR-2} score, which, given a time limit of $\tau$, is the average runtime with a penalty of $2 \tau$ for timeouts~\cite{FroleyksHIJS21}.

In this paper, we present a novel benchmark selection approach based on active learning.
Our approach can predict the rank of a new solver with high accuracy in only a fraction of the time needed to evaluate the complete benchmark.
Definition~\ref{def:new-solver-problem} specifies the problem we address.
Note that our scenario assumes knowing the runtimes of all solvers, except the new one, on all instances.
One could also imagine a collaborative filtering scenario, where runtimes are only partially known~\cite{misir2017data,misir2017alors}.

\begin{definition}[New-Solver Problem]
  Let us consider a set of solvers~$\mathcal{A}$, instances~$\mathcal{I}$, and runtimes~${r\!: \mathcal{A} \times \mathcal{I} \rightarrow \left[0, \tau\right]}$ with each solver having a runtime limit of $\tau$. 
  For a new solver $\hat{a} \notin \mathcal{A}$, incrementally select benchmark instances from $\mathcal{I}$ to maximize the confidence in predicting the rank of $\hat{a}$ while minimizing the total benchmark runtime.
  \label{def:new-solver-problem}
\end{definition}

The approach presented in this paper meets several criteria for benchmarking that are considered desirable (cf.~Table~\ref{tab:requirements}).
Rather than outputting a binary classification, namely whether the new solver is worse than an existing solver or not, we provide a \emph{scoring} function that shows by which margin a solver is worse and how similar it is to existing solvers.
In particular, our approach enables \emph{ranking} the new solver amidst a set of existing solvers.
We show that for ranking, it is not necessary to predict the exact per-instance runtime of the solvers, which would be a more challenging task.
Furthermore, we optimize the \emph{runtime} required for our strategy to arrive at its conclusion.
To this end, we use instance \emph{features} and known solver \emph{runtimes}.
Moreover, we select instances \emph{non-randomly} and \emph{incrementally}.
In particular, we consider runtime information from already done experiments when choosing the next.
By doing so, we can control the properties of the benchmarking approach, such as its required runtime.
Our approach is \emph{scalable} in that it ranks a new solver $\hat{a}$ among any number of known solvers $\mathcal{A}$.
In particular, we only subsample the benchmark once instead of comparing pairwise against each other solver~\cite{MatriconAFSH21}.

We evaluated our approach using the SAT Competition~2022 Anniversary Track dataset~\cite{sat2022}, consisting of 5355~instances and runtimes of 28~solvers.
Cross-validation was performed by treating each solver once as the new solver and learning to predict the PAR-2 rank of that solver.
On average, our predictions achieve an accuracy of approximately \SI{92}{\%} with only approximately \SI{10}{\%} of the runtime required to evaluate these solvers on the complete set of instances.
Our entire source code\footnote{\url{https://github.com/mathefuchs/al-for-sat-solver-benchmarking}} and experimental data\footnote{\url{https://github.com/mathefuchs/al-for-sat-solver-benchmarking-data}} are available on GitHub.

This article is an extended and refined version of our conference paper with the title ``Active Learning for SAT Solver Benchmarking''~\cite{fuchs2023active}.
In particular, we significantly extended the analysis of runtime-prediction approaches, which determined our chosen solver runtime model for instance selection (cf.~Section~\ref{sec:exdesign:disc-pred}).
Furthermore, we expanded the evaluation of which benchmark instances are selected by our approach (cf.~Section~\ref{sec:eval:instance}).

\section{Related Work}

Benchmarking is of great interest in numerous research areas and represents an active field of research in its own right.
Studies have shown that the compilation of benchmark instances poses a number of challenges.
The use of biased benchmarks can easily lead to fallacious interpretations~\cite{abs-2107-07002}.
Benchmarking also involves a number of interchangeable elements, including the performance measures used, the way the measures are aggregated, and the way missing values are handled.
Questionable research practices could alter these elements a-posteriori in order to meet expectations, thereby skewing the results~\cite{NiesslHWCB22}.

The following sections present a discussion of related work from the areas of static benchmark selection, algorithm configuration, incremental benchmark selection, and active learning.
Table~\ref{tab:requirements} compares the most relevant approaches, which all pursue slightly different goals.
Thus, our approach is \emph{not} a general improvement over the others but the only one fully aligned with Definition~\ref{def:new-solver-problem}.

\subsection{Static Benchmark Selection}

Benchmark selection is essential for competitions such as the SAT Competition.
In such competitions, the organizers define the rules for composing the benchmarks.
These selection strategies are primarily static, in that they do not depend on particular solvers to distinguish.
Balint et al. provide an overview of benchmark-selection criteria in different solver competitions~\cite{balint2015overview}.
Froleyks et al. describe benchmark selection in recent SAT~competitions~\cite{FroleyksHIJS21}.
Manthey and MÃ¶hle propose an approach to remove redundancy from competition benchmarks~\cite{manthey2016better}, considering feature equivalence of formulas.
M{\i}s{\i}r also presents a feature-based approach to reduce benchmarks, using matrix factorization and clustering~\cite{misir2021benchmark}.

Hoos et al.~\cite{HoosKSS13} discuss desirable properties of SAT benchmark instances.
They identify three key selection criteria: instance variety to avoid over-fitting, adapted instance hardness (not too easy but also not too hard), and the avoidance of duplicate instances. 
To filter instances that are too similar, they employ a distance-based approach with the SATzilla features~\cite{XuHHL08,features}.
It should be noted, however, that the approach does not optimize for benchmark \emph{runtime}.
Instead, instances are selected \emph{at random}, with the exception of constraints on instance hardness and feature distance.

\begin{table}[tbp]
  \centering
  \caption{Comparison of features of our benchmark-selection approach, the static benchmark-selection approach by Hoos~et~al.~\cite{HoosKSS13}, the algorithm configuration system SMAC~\cite{HutterHL11}, and the active-learning approaches by Matricon et al.~\cite{MatriconAFSH21}.
  }
  \label{tab:requirements}
  \begin{tabular}{
    m{0.2752\textwidth}
    >{\centering\arraybackslash}m{0.1376\textwidth}
    >{\centering\arraybackslash}m{0.1376\textwidth}
    >{\centering\arraybackslash}m{0.1376\textwidth}
    >{\centering\arraybackslash}m{0.1376\textwidth}
  }
    \toprule
    Feature & Hoos~\cite{HoosKSS13} & SMAC~\cite{HutterHL11} & Matricon~\cite{MatriconAFSH21} & Our approach \\
    \midrule
    Ranking/Scoring & \cmark & \xmark & (\cmark) & \cmark \\
    Runtime Minimization & \xmark & \cmark & \cmark & \cmark \\
    Incremental/Non-Random & \xmark & \xmark & \cmark & \cmark \\
    Scalability & \cmark & \cmark & \xmark & \cmark \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Algorithm Configuration}

Further related work can be found within the field of algorithm configuration~\cite{HoosHL21,Stutzle0P22}, as exemplified by the configuration system SMAC~\cite{HutterHL11}.
Thereby, the goal is to tune SAT solvers for a given sub-domain of problem instances.
While this task differs from our goal, for instance, in that we do not need to navigate the configuration space, there are similarities to our approach as well.
For example, SMAC employs an iterative, model-based selection procedure similar to our approach, but this is for selecting configurations rather than instances.
However, an algorithm configurator cannot be used to \emph{rank} or \emph{score} a new solver since algorithm configuration is designed to identify the optimal configuration.
Furthermore, while a model-based selection strategy is employed to sample configurations, instance selection is made \emph{randomly}, i.e., without building a model over instances.

\subsection{Incremental Benchmark Selection}

Matricon~et~al. present an incremental benchmark selection approach~\cite{MatriconAFSH21}.
Their \emph{per-set efficient algorithm selection problem} (PSEAS) is similar to our \emph{New-Solver Problem}, as given in Definition~\ref{def:new-solver-problem}.
To differentiate between \emph{a pair of} SAT solvers, the authors propose an iterative approach to select a solver-specific subset of instances until a desired confidence level is reached.
This is achieved by calculating a scoring metric for all unselected instances, running the experiment with the highest score, and updating the confidence.
Their approach ticks off most of the desired features in Table~\ref{tab:requirements}.
However, the approach only compares solvers binarily rather than providing a \emph{scoring}.
Consequently, it is unclear how similar two given solvers are or on which instances they behave similarly.
Furthermore, a significant shortcoming is the lack of \emph{scalability} with the number of solvers.
Comparing only pairs of solvers, evaluating a new solver with their method requires sampling a separate benchmark for each existing solver.
In contrast, our approach allows comparing a new solver against a set of existing solvers by sampling only one benchmark.

\subsection{Active Learning}

In passive machine learning, prediction models are trained on datasets with given instance labels (cf.~Fig.~\ref{fig:passive}).
In contrast, active learning (AL) begins with no or little labeled data.
AL then repeatedly selects interesting problem instances for which to acquire labels, gradually improving the prediction model (cf.~Fig.~\ref{fig:activepool}).
AL methods are particularly advantageous when acquiring labels is computationally expensive, such as obtaining solver runtimes.
Without AL methods, it is not evident which instances to label and which not.
The first goal is to maximize the utility of instances to our model, which is measured by the rank prediction accuracy.
The second goal is to minimize the cost of label acquisition, which is measured by the instance's predicted runtime.
Consequently, the overall objective is to develop an accurate prediction model without the necessity of labeling each data point.

Rubens et.~al.~\cite{RubensESK15} survey active-learning advances.
While synthesis-based AL methods~\cite{0001AEMN22,GarzonMG22,2019gaal} generate instances for labeling, pool-based methods~\cite{GolbandiKL11,HarpaleY08,KorenBV09} rely on a fixed set of unlabeled instances to sample from.
Recent synthesis-based methods within the field of SAT solving demonstrate the generation of problem instances with desired properties~\cite{0001AEMN22,GarzonMG22}.
However, this objective is distinct from ours.
While those approaches aim to generate instances on which a solver is either effective or ineffective, our objective is to predict whether a solver is effective or ineffective on an \emph{existing} benchmark.
In their work, Volpato and Guangyan employ pool-based AL to train an instance-specific algorithm selector~\cite{volpato2019active}.
Rather than benchmarking a solver's overall performance, their objective is to recommend the best solver for each SAT instance from a set of available solvers.

\begin{figure}[tbp!]
  \centering
  \begin{tabular}[c]{ccc}
  \begin{subfigure}[b]{0.24\textwidth}
  \centering
  \resizebox{!}{2.75cm}{
  \begin{tikzpicture}
  \draw[fill=black]  (-2.8,3.7) ellipse (0.1 and 0.1);
  \draw[fill=black]  (-2.6,3.4) arc (0:180:0.2);
  \node[right] at (-5.9,3.6) {Agent (Solver)};
  \draw[fill=white]  (-3.3,2.8) rectangle (-2.5,2.2);
  \draw[fill=white]  (-3.2,2.7) rectangle (-2.4,2.1);
  \draw[fill=white]  (-3.1,2.6) rectangle (-2.3,2);
  \node[right] at (-5.9,2.4) {Training Data};
  \node (v1) at (-2.8,3.4) {};
  \node (v2) at (-2.8,2.8) {};
  \draw[->]  (v1) edge (v2);
  \node[right] at (-2.8,3.1) {$x, y$};
  \node (v3) at (-2.8,2) {};
  \node (v4) at (-2.8,1.4) {};
  \draw[->]  (v3) edge (v4);
  \draw  (-3.1,1.4) rectangle (-2.5,1);
  \draw  (-2.4,3.2) rectangle (-2.4,3.2);
  \node[right] at (-5.9,1.2) {Learning Alg.};
  \node (v5) at (-2.8,1) {};
  \node (v6) at (-2.8,0.4) {};
  \draw[->]  (v5) edge (v6);
  \node[right] at (-5.9,0.25) {Predic. Function};
  \node at (-2.8,0.25) {$f$};
  \end{tikzpicture}
  }
  \caption{Passive.}
  \label{fig:passive}
  \end{subfigure}
  &
  \begin{subfigure}[b]{0.33\textwidth}
  \centering
  \resizebox{!}{2.75cm}{
  \begin{tikzpicture}
  \draw[fill=black]  (-2.8,3.7) ellipse (0.1 and 0.1);
  \draw[fill=black]  (-2.6,3.4) arc (0:180:0.2);
  \node[right] at (-5.9,3.6) {Agent (Solver)};
  \draw[fill=white]  (-3.3,2.8) rectangle (-2.5,2.2);
  \draw[fill=white]  (-3.2,2.7) rectangle (-2.4,2.1);
  \draw[fill=white]  (-3.1,2.6) rectangle (-2.3,2);
  \node[right] at (-5.9,2.4) {Training Data};
  \node (v1) at (-2.8,3.4) {};
  \node (v2) at (-2.8,2.8) {};
  \draw[->]  (v1) edge (v2);
  \node[right] at (-2.8,3.1) {$x, y$};
  \node (v3) at (-2.8,2) {};
  \node (v4) at (-2.8,1.4) {};
  \draw[->]  (v3) edge (v4);
  \draw  (-3.1,1.4) rectangle (-2.5,1);
  \draw  (-2.4,3.2) rectangle (-2.4,3.2);
  \node[right] at (-5.9,1.2) {Learning Alg.};
  \node (v5) at (-2.8,1) {};
  \node (v6) at (-2.8,0.4) {};
  \draw[->]  (v5) edge (v6);
  \node[right] at (-5.9,0.25) {Predic. Function};
  \node at (-2.8,0.25) {$f$};
  \node[right] at (-1.2,2.4) {Pool};
  \draw[->, densely dashed] (-2.4,1.2) arc (-90:0:0.8);
  \draw[->, densely dashed] (-1.6,2.8) arc (0:90:0.8);
  \node[right] at (-1.6,3.135) {$x, ?$};
  \draw  (-1.6,2.4) ellipse (0.4 and 0.3);
  \draw[fill=black]  (-1.725,2.5) ellipse (0.07 and 0.07);
  \draw[fill=black]  (-1.625,2.25) ellipse (0.07 and 0.07);
  \draw[fill=black]  (-1.425,2.4) ellipse (0.07 and 0.07);
  \end{tikzpicture}
  }
  \caption{Active Pool-based.}
  \label{fig:activepool}
  \end{subfigure}
  &
  \begin{subfigure}[b]{0.33\textwidth}
  \centering
  \resizebox{!}{2.75cm}{
  \begin{tikzpicture}
  \draw[fill=black]  (-2.8,3.7) ellipse (0.1 and 0.1);
  \draw[fill=black]  (-2.6,3.4) arc (0:180:0.2);
  \node[right] at (-5.9,3.6) {Agent (Solver)};
  \draw[fill=white]  (-3.3,2.8) rectangle (-2.5,2.2);
  \draw[fill=white]  (-3.2,2.7) rectangle (-2.4,2.1);
  \draw[fill=white]  (-3.1,2.6) rectangle (-2.3,2);
  \node[right] at (-5.9,2.4) {Training Data};
  \node (v1) at (-2.8,3.4) {};
  \node (v2) at (-2.8,2.8) {};
  \draw[->]  (v1) edge (v2);
  \node[right] at (-2.8,3.1) {$x, y$};
  \node (v3) at (-2.8,2) {};
  \node (v4) at (-2.8,1.4) {};
  \draw[->]  (v3) edge (v4);
  \draw  (-3.1,1.4) rectangle (-2.5,1);
  \draw  (-2.4,3.2) rectangle (-2.4,3.2);
  \node[right] at (-5.9,1.2) {Learning Alg.};
  \node (v5) at (-2.8,1) {};
  \node (v6) at (-2.8,0.4) {};
  \draw[->]  (v5) edge (v6);
  \node[right] at (-5.9,0.25) {Predic. Function};
  \node at (-2.8,0.25) {$f$};
  \node[right, label={[align=left]Gen.\\Model}] at (-0.75,1.88) {};
  \node[right] at (-1.3,3.135) {$x, ?$};
  \draw[->, densely dashed] (-2.4,1.2) arc (-90:90:1.2);
  \end{tikzpicture}
  }
  \caption{Active Synthesis-based.}
  \label{fig:activesynth}
  \end{subfigure}
  \end{tabular}
  
  \caption{Types of machine learning (depiction inspired by Rubens~et.al.~\cite{RubensESK15}).}
  \label{fig:learning}
\end{figure}


\section{Active Learning for SAT Solver Benchmarking}
\label{sec:main}

Algorithm~\ref{algALBenchmark} outlines our benchmarking framework. 
Given a set of solvers~$\mathcal{A}$, instances~$\mathcal{I}$ and runtimes~$r$, we first initialize a prediction model~$\mathcal{M}$ for the new solver $\hat a \not\in \mathcal{A}$ (Line~1).
The prediction model~$\mathcal{M}$ is used to select instances repeatedly (Line~4) for benchmarking the new solver $\hat a$ (Line~5). 
The acquired result is subsequently employed to update the prediction model, denoted by~$\mathcal{M}$ (Line~7). 
When the stopping criterion is met (Line~3), the benchmarking loop is terminated and the final score of $\hat{a}$ is estimated (Line~8). 
Algorithm~\ref{algALBenchmark} returns this score, along with the acquired instances and runtime measurements. 

Section~\ref{sec:main:model} provides an overview of the underlying prediction model~$\mathcal{M}$ and outlines the methodology for deriving a solver ranking from it.
Section~\ref{sec:main:selection} presents the criteria for selecting instances.
Finally, Section~\ref{sec:main:stopping} presents potential stopping conditions.

\begin{algorithm}[t]
  \caption{Incremental Benchmarking Framework}
  \label{algALBenchmark}

  \KwIn{Solvers $\mathcal{A}$, Instances $\mathcal{I}$, Runtimes $r : \mathcal{A} \times \mathcal{I} \rightarrow [0, \tau]$, Solver $\hat{a}$}
  \KwOut{Predicted Score of $\hat{a}$, Measured Runtimes $\mathcal{R}$}

  \BlankLine

  $\mathcal{M} \leftarrow \operatorname{initModel}\left(\mathcal{A},\, \mathcal{I},\, r,\, \hat{a}\right)$ \tcp*{cf. Section~\ref{sec:main:model}}
  
  \BlankLine
  $\mathcal{R} \leftarrow \emptyset$ \;
  \While(\tcp*[f]{cf. Section~\ref{sec:main:stopping}}){$\operatorname{not} \operatorname{stop}\left(\mathcal{M}\right)$}{
    $e \leftarrow \operatorname{selectNextInstance}\left(\mathcal{M}\right)$ \tcp*{cf. Section~\ref{sec:main:selection}}

    $t \leftarrow \operatorname{runExperiment}\left(\hat{a},\,  e\right)$  \tcp*{Runs $\hat{a}$ on $e$ with timeout $\tau$}

    $\mathcal{R} \leftarrow \mathcal{R} \cup \left\lbrace (e,\, t) \right\rbrace$

    \BlankLine
    $\operatorname{updateModel}\left(\mathcal{M},\, \mathcal{R}\right)$ \tcp*{cf. Section~\ref{sec:main:model}}
  }
  $s_{\hat a} \leftarrow \operatorname{predictScore}(\mathcal{M})$ \tcp*{cf. Section~\ref{sec:main:model}}
  
  \BlankLine
  \Return $(s_{\hat a}, \mathcal{R})$
\end{algorithm}


\subsection{Solver Model}
\label{sec:main:model}

The model $M$ provides a runtime-label prediction function $f : \mathcal{\hat A} \times \mathcal{I} \rightarrow \mathbb{R}$ for all solvers $\mathcal{\hat A} := \mathcal{A} \cup \lbrace \hat a \rbrace$.
This prediction function powers instance selection as described in Section~\ref{sec:main:selection}.
During model updates (Algorithm~\ref{algALBenchmark},~Line~7), the prediction function $f$ is trained to predict a transformed version of the acquired runtime dataset~$\mathcal{R}$.
We describe the transformation of the runtime dataset in the subsequent section.
The features described in Section~\ref{sec:exdesign:data} serve as the input to the model.
Furthermore, it should be noted that a new prediction model is constructed in each iteration, as the runtime of the experiments (Line~5) dominates the training time of the model by a significant margin.
Finally, the score of the new solver, denoted by $\hat a$, is predicted with the prediction function~$f$ (Line~8).

\subsubsection{Runtime Transformation}

For the prediction model~$M$, we transform the real-valued runtimes into discrete runtime labels on a per-instance basis.
For each instance $e \in \mathcal{I}$, we use a clustering algorithm to assign the runtimes in $\bigl\{ r(a, e) \mid a \in \mathcal{A} \bigr\}$ to one of $k$ clusters $C_1, \dots, C_k$ such that the fastest runtimes for the instance $e$ are in cluster $C_1$ and the slowest are in cluster $C_{k-1}$.
Timeouts $\tau$ always form a separate cluster $C_{k}$.
The runtime transformation function $\gamma_k : {\mathcal{A} \times \mathcal{I}} \rightarrow \left\lbrace 1, \dots, k \right\rbrace$ is then specified as follows:
%
$$\gamma_k(a, e) = j ~\Leftrightarrow~ r(a, e) \in C_j$$
%
In the context of such a clustering, for an instance $e \in \mathcal{I}$, a solver $a \in \mathcal{A}$ is defined as belonging to the $\gamma_k(a, e)$-fastest solvers on instance $e$. 
%
Empirical studies on portfolio solvers have demonstrated that discretization is an effective approach in practice~\cite{CollauttiMMO13,NgokoCT19}.
Section~\ref{sec:exdesign:disc-pred} presents a detailed analysis of the runtime transformation we designed for our approach.
The analysis demonstrates that our approach achieves higher accuracy for discrete runtime labels than for raw runtimes.

\subsubsection{Ranking Solvers}

To determine the rank of solvers, we apply PAR-2 scoring using the discrete runtime labels $\gamma_k(a, e)$.
The adapted scoring function $s_k : \mathcal{A} \rightarrow [1, 2 \cdot k]$ induces a ranking among solvers and is depicted in Equation~\eqref{eq:rankingeq}.
%
\begin{align}
  s_k(a) := \frac{1}{|\mathcal{I}|} \sum_{e \in \mathcal{I}} \gamma'_k(a, e)
  &&
  \gamma'_k(a, e) := \begin{cases}
    2 \cdot \gamma_k(a, e)   & \text{if } \gamma_k(a, e) = k\\
  \gamma_k(a, e)  & \text{otherwise}
  \end{cases}
  \label{eq:rankingeq}
\end{align}
%
\subsection{Instance Selection}
\label{sec:main:selection}

Selecting an instance based on the model is a core functionality of our framework (cf.~Algorithm~\ref{algALBenchmark}, Line~4).
In this section, we introduce two instance sampling strategies, one that minimizes uncertainty and one that maximizes information gain.
Both strategies use the model's label-prediction function $f$ and are inspired by existing work within the realms of active learning~\cite{settles2009active}.
These methods require the model's predictions to include probabilities for the $k$ discrete runtime labels.
Let \mbox{$f' : \mathcal{\hat A} \times \mathcal{I} \rightarrow \left[0, 1\right]^k$} denote this modified prediction function.
In the following, the set $\tilde{\mathcal{I}} \subseteq \mathcal{I}$ denotes the instances that have already been sampled.

\paragraph{Uncertainty Sampling}

The uncertainty sampling strategy selects the instance that is closest to the model's decision boundary.
This is achieved by selecting the instance $e \in \mathcal{I} \setminus \tilde{\mathcal{I}}$ that minimizes $U(e)$, which is specified by the following equation.
%
\begin{equation*}
  \operatorname{U}(e) := \left\lvert \frac{1}{k} - \max_{n \in \left\lbrace 1, \dots, k \right\rbrace} f'\!\left(\hat{a}, e\right)_{n} \right\rvert
\end{equation*}

\paragraph{Information-Gain Sampling}

The information-gain sampling strategy selects the instance with the highest expected entropy reduction regarding the runtime labels of the instance.
To be more precise, we select the instance $e \in \mathcal{I} \setminus \tilde{\mathcal{I}}$ that maximizes $IG(e)$, which is specified in the following equation.
%
\begin{equation*}
  \operatorname{IG}(e) := \operatorname{H}(e) - \sum_{n = 1}^{k} f'(\hat{a}, e)_{n} \operatorname{\hat H}_n(e)
\end{equation*}
%
In the equation, $\operatorname{H}(e)$ denotes the entropy of the runtime labels $\gamma(a, e)$ over all $a \in \mathcal{A}$ and $\operatorname{H}(e, n)$ denotes the entropy of these labels plus $n$ as the runtime label for $\hat{a}$.
The term $\operatorname{\hat H}_n(e)$ is computed for every possible runtime label $n \in \{1, \dots, k\}$.
By maximizing the information gain, instances are selected that identify solvers with similar behavior.

\subsection{Stopping Criteria}
\label{sec:main:stopping}

In this section, we present the two dynamic stopping criteria employed in our experiments: the Wilcoxon and the ranking stopping criterion (cf. Algorithm~\ref{algALBenchmark}, Line~3).

\paragraph{Wilcoxon Stopping Criterion}

The Wilcoxon stopping criterion is employed to halt the active-learning process based on the confidence that the predicted runtime labels of the new solver are sufficiently different from those of existing solvers.
This criterion is loosely inspired by Matricon et.~al.~\cite{MatriconAFSH21}.
The average $p$-value $W_{\hat{a}}$ of a Wilcoxon signed-rank test $\operatorname{w}(S,P)$, as given by the following equation, of the two runtime label distributions ${S=\{ \gamma(a, e) \mid e \in \mathcal{I} \}}$ for an existing solver $a$ and \mbox{$P=\{ f(\hat a, e) \mid e \in \mathcal{I} \}$} of the new solver $\hat{a}$ is used to assess the statistical significance of the predicted runtime labels.
%
\begin{equation*}
  W_{\hat{a}} := \frac{1}{\lvert \mathcal{A} \rvert} \sum_{a \in \mathcal{A}} \operatorname{w}(S, P)
\end{equation*}
%
In order to improve the stability of this criterion, an exponential moving average is employed to mitigate the impact of outliers, with the process terminated when the value of $W^{(i)}_{\exp}$ drops below a fixed threshold.
%
\begin{align*}
  W_{\exp}^{\left(0\right)} &:= 1\\
  W_{\exp}^{\left(i\right)} &:= \beta W_{\hat{a}} + \left(1 - \beta\right) W_{\exp}^{\left(i - 1\right)}
\end{align*}

\paragraph{Ranking Stopping Criterion}

In contrast, the ranking stopping criterion is less sophisticated than the previous one.
In this case, the active learning process is terminated if the ranking induced by the model's predictions (Equation~\ref{eq:rankingeq}) remains unchanged within the last~$l$~iterations.
However, it should be noted that the concrete values of the predicted score $s_{\hat a}$ may still change.
Nevertheless, in this case, the induced ranking is of sole interest.

\section{Experimental Design}
\label{sec:exdesign}

In light of the preceding instantiations of Algorithm~\ref{algALBenchmark}, this section outlines the experimental design employed in the presented study.
This includes an overview of the evaluation framework, the datasets employed, the hyperparameter selections, and the implementation details.

\subsection{Evaluation Framework}
\label{sec:exdesign:eval}

\begin{algorithm}[tb]
  \caption{Evaluation Framework}
  \label{alg:eval}

  \KwIn{Solvers $\mathcal{A}$, Instances $\mathcal{I}$, Runtimes $r : \mathcal{A} \times \mathcal{I} \rightarrow [0, \tau]$}
  \KwOut{Average Ranking Accuracy $\bar{O}_{\operatorname{acc}}$, Average Fraction of Runtime $\bar{O}_{\operatorname{rt}}$}
  \BlankLine

  $O \leftarrow \emptyset$
  % \BlankLine

  \For{$\hat{a} \in \mathcal{A}$}{
    $\mathcal{A}' \leftarrow \mathcal{A} \setminus \left\lbrace \hat{a} \right\rbrace$ \;
    $(s_{\hat a}, \mathcal{R}) \leftarrow \operatorname{runALAlgorithm}(\mathcal{A}', \mathcal{I}, r, \hat{a})$ \tcp*{Refer to Algorithm~\ref{algALBenchmark}}

  \BlankLine
  \tcp{Determine Ranking Accuracy}
    $O_{\operatorname{acc}} \leftarrow 0$ \;    
    \For{$a \in \mathcal{A}$}{
      \If{$\bigl(s_k(a) - s_{\hat a}\bigr) \cdot \bigl(\operatorname{par_2}(a) - \operatorname{par_2}(\hat a)\bigr) > 0$}{
           $O_{\operatorname{acc}} \leftarrow O_{\operatorname{acc}} + \frac{1}{|\mathcal{A}|}$ \;
      }
    }
  
  \BlankLine
  \tcp{Determine Runtime Fraction}
  $r \leftarrow \sum\limits_{e \in \mathcal{I}} r(\hat a, e)$\;
  $O_{\operatorname{rt}} \leftarrow 0$\;
    \For{$e \in \mathcal{I}$}{
    \If{$\exists t, (e,t) \in \mathcal{R}$}{
      $O_{\operatorname{rt}} \leftarrow O_{\operatorname{rt}} + \frac{t}{r}$\;
    }
  }
  
  % \BlankLine
    $O \leftarrow O \cup \bigl\{ ( O_{\operatorname{acc}},\, O_{\operatorname{rt}} ) \bigr\}$
  }

  % \BlankLine
  $\bigl( \bar{O}_{\operatorname{acc}}, \bar{O}_{\operatorname{rt}} \bigr) \leftarrow \operatorname{average}(O)$ \;
  
  % \BlankLine
  \Return $\bigl( \bar{O}_{\operatorname{acc}}, \bar{O}_{\operatorname{rt}} \bigr)$
\end{algorithm}

As stated in the Introduction, this work addresses the \emph{New-Solver Problem} (cf.~Definition~\ref{def:new-solver-problem}).
As described in Section~\ref{sec:main:model}, a prediction model provides us with an estimated scoring value for the new solver~$\hat{a}$.
To evaluate the efficacy of a concrete instantiation of Algorithm~\ref{algALBenchmark}, which entails a specific choice for all subroutines, we perform cross-validation on our set of solvers.

This is demonstrated in Algorithm~\ref{alg:eval}.
Each solver assumes the role of the new solver~$\hat{a}$ once (Line~2) and is consequently excluded from the set of solvers $\mathcal{A}$ (Line~3).
After running our active-learning framework for solver~$\hat{a}$ (Line~4), we compute the value of both our optimization goals, i.e., ranking accuracy and runtime.
The \emph{ranking accuracy} $O_{\operatorname{acc}} \in \left[0, 1\right]$ (higher is better) is defined as the fraction of correctly ranked pairs of elements, namely, $\left(\hat{a}, a\right)$, regarding the ground-truth scoring function $\operatorname{par_2}$, where $a$ belongs to the set of all solvers $\mathcal{A}$ (Lines~5-8).
The \emph{fraction of runtime} required by the algorithm to reach its conclusion is represented by $O_{\operatorname{rt}}$, which lies within the interval $[0, 1]$ (lower is better).
This metric compares the accumulated runtimes over the sampled instances to the accumulated runtimes over all instances in the dataset (Lines~9-13).
Once all cross-validation results have been collected (Line~14), the output metrics are averaged (Line~15).

The procedure is designed to identify an approach that maximizes Equation~\eqref{eq:opt}, in which $\delta \in \left[0, 1\right]$ is used for linear weighting, with weighting factor $\delta \in \left[0, 1\right]$, between the two optimization goals $O_{\operatorname{acc}}$ and $O_{\operatorname{rt}}$.
The plotting of the approaches that maximize $O_\delta$ for all $\delta \in \left[0, 1\right]$ on an $O_{\operatorname{rt}}$-$O_{\operatorname{acc}}$-diagram provides us with a Pareto front of the best approaches for different optimization-goal weightings.
%
\begin{equation}
  O_\delta := \delta O_{\operatorname{acc}} + \left(1 - \delta\right) \left(1 - O_{\operatorname{rt}}\right) \enspace \textrm{,}
  \label{eq:opt}
\end{equation} 
%

\subsection{Data}
\label{sec:exdesign:data}

In our experiments, we work with the dataset of the SAT~Competition~2022 Anniversary Track~\cite{sat2022}.
The dataset consists of 5355 instances with corresponding runtime data of 28 sequential SAT solvers.
We also use a database of 56 instance features\footnote{\url{https://benchmark-database.de/getdatabase/base_db}} from the Global Benchmark Database~(GBD) by Iser~et~al.~\cite{IserS18}.
They include instance size features and node distribution statistics for several graph representations of SAT instances, and are primarily inspired by the SATzilla~2012 features described in~\cite{features}.
All features are numeric and have no missing values.
We drop 10 of the 56 features due to zero variance.
In total, the prediction models have access to 46 instance features and 27 out of 28 runtime features, excluding the respective new solver~$\hat{a}$.

In addition, we retrieve instance family information\footnote{\url{https://benchmark-database.de/getdatabase/meta_db}} to evaluate the composition of our sampled benchmarks.
Instance families consist of instances from the same application domain, such as planning, cryptography, etc., and are a valuable tool for analyzing solver performance.
%
For hyperparameter tuning, we randomly sample \SI{10}{\%} of the complete set of 5355 instances with stratification regarding the instance family.
All instance families that are too \emph{small}, i.e., \SI{10}{\%} of them correspond to less than one instance, are put into a meta-family for stratification.
This \emph{tuning dataset} allows a more extensive exploration of the hyperparameter space.

\subsection{Hyperparameters}
\label{sec:exdesign:hyper}

Given the Algorithm~\ref{algALBenchmark}, there are several possible instantiations for the three subroutines \emph{ranking}, \emph{selection}, and \emph{stopping}.
These experimental configurations are described in the following.

\subparagraph{Ranking}

Regarding \emph{ranking} (cf.~Section~\ref{sec:main:model}), we experiment with the following instantiations.

\begin{itemize}\setlength{\itemsep}{1pt}
  \item Observed PAR-2 ranking of already sampled instances
  \item Predicted runtime-label ranking
  \vspace*{-1ex}
  \begin{itemize}\setlength{\itemsep}{1pt}
    \item
    History size: Consider the latest 1, 10, 20, 30, or 40 predictions within a voting approach for stability.
    The latest $x$ predictions for each instance vote on the instance's winning label.
    \item
    Fallback threshold: If the difference of scores between the new solver~$\hat{a}$ and another solver drops below \SI{0.01}, \SI{0.05}, or \SI{0.1}, use the partially observed PAR-2 ranking as a tie-breaker.
  \end{itemize}
\end{itemize}

\subparagraph{Selection}

We experiment with the following instantiations of the instance \emph{selection} (cf. Section~\ref{sec:main:selection}).
Since the potential runtime of the experiments is orders of magnitude larger than the model update time, we consider incrementing our benchmark one instance at a time rather than using batches, which is also proposed in recent advances in active learning~\cite{SinhaED19,2019gaal}.
A drawback of this is the lack of parallel execution of runtime experiments.

\begin{itemize}\setlength{\itemsep}{1pt}
  \item Random sampling 
  \item Uncertainty sampling
  \vspace*{-1ex}
  \begin{itemize}\setlength{\itemsep}{1pt}
    \item Fallback threshold: Use random sampling for the first \SI{0}{\%}, \SI{5}{\%}, \SI{10}{\%}, \SI{15}{\%}, or \SI{20}{\%} of instances to explore the instance space.
    \item Runtime scaling: Whether to normalize uncertainty scores per instance by the average runtime of solvers on it or use the absolute values.
  \end{itemize}

  \item Information-gain sampling
  \vspace*{-1ex}
  \begin{itemize}
    \item Fallback threshold: Use random sampling for the first \SI{0}{\%}, \SI{5}{\%}, \SI{10}{\%}, \SI{15}{\%}, or \SI{20}{\%} of instances to explore the instance space.
    \item Runtime scaling: Whether to normalize information-gain scores per instance by the average runtime of solvers on it or use the absolute values.
  \end{itemize}
\end{itemize}

\subparagraph{Stopping}

We evaluate the following \emph{stopping} (cf. Section~\ref{sec:main:stopping}) criteria.

\begin{itemize}\setlength{\itemsep}{1pt}
  \item Subset-size stopping criterion, using \SI{10}{\%} or \SI{20}{\%} of instances
  \item Ranking stopping criterion
  \vspace*{-1ex}
  \begin{itemize}\setlength{\itemsep}{1pt}
    \item Minimum amount: Sample at least \SI{2}{\%}, \SI{8}{\%}, \SI{10}{\%}, or \SI{12}{\%} of instances before applying the criterion.
    \item Convergence duration: Stop if the predicted ranking stays the same for a number of sampled instances equal to \SI{1}{\%} or \SI{2}{\%} of all instances.
  \end{itemize}
  \item Wilcoxon stopping criterion
  \vspace*{-1ex}
  \begin{itemize}\setlength{\itemsep}{1pt}
    \item Minimum amount: Sample at least \SI{2}{\%}, \SI{8}{\%}, \SI{10}{\%}, or \SI{12}{\%} of instances before applying the criterion.
    \item Average of $p$-values to drop below: \SI{5}{\%}.
    \item Exponential-moving average: Incorporate previous significance values by using an EMA with $\beta = 0.1$ or $\beta = 0.7$.
  \end{itemize}
\end{itemize}

\subsection{Runtime Discretization and Prediction}
\label{sec:exdesign:disc-pred}

One of the most important parts of our framework is the choice of solver runtime model to inform active learning selection decisions.
In this section, we detail how clustering solver runtimes can lead to nuanced runtime labels, and which machine learning models are best at predicting them.

As a baseline for improvement, Table~\ref{tab:regression} shows the runtime prediction performance of two regression models, a random forest and a multilayer perceptron, using 46 instance features and 27 known solver runtimes.
Both models were chosen for their robustness and ability to capture nonlinear dependencies~\cite{breiman2001random}, and both perform poorly, as evidenced by the magnitudes of the mean regression errors.
An RMSE error of about \SI{2000}{s} is quite poor compared to solver runtimes that range from 0 to \SI{5000}{s}.
Therefore, we decided to discretize the runtimes.
%
\begin{table}[b]
  \centering
  \caption{Regression performance of models trained on 46 instance and 27 runtime features. We report the RMSE and MAE metrics on the test sets.}
  \label{tab:regression}
  \begin{tabular}{
    >{\raggedleft\arraybackslash}m{0.33855\textwidth}
    >{\centering\arraybackslash}m{0.01\textwidth}
    >{\centering\arraybackslash}m{0.2562\textwidth}
    >{\centering\arraybackslash}m{0.2562\textwidth}
  }
    \toprule
    {Regression Models} & & {Avg. RMSE ($\pm$ std)} & {Avg. MAE ($\pm$ std)} \\
    \midrule
    Random Forest & & \SI{1929.84}{s} ($\pm$ \SI{1161.15}{s}) & \phantom{0}\SI{957.36}{s} ($\pm$ \SI{297.21}{s}) \\[0.4ex]
    Multilayer Perceptron & & \SI{2275.20}{s} ($\pm$ \SI{1545.01}{s}) & \SI{1284.30}{s} ($\pm$ \SI{390.57}{s}) \\
    \bottomrule
  \end{tabular}
\end{table}

Table~\ref{tab:timeout-prediction} shows the performance of different models in predicting timeouts.
While the second column shows the performance of models trained on the 46 instance features, the third column shows the performance of models trained on the 46 instance features and the 27 runtime features.
While the Quadratic DA model performs poorly on the 46 instance features, it performs well on the 46 instance features and the 27 runtime features.
However, the best prediction performance is achieved by a stacking ensemble~\cite{wolpert1992stacked}, which combines quadratic discriminant analysis~\cite{tharwat2016linear} and a random forest classifier~\cite{breiman2001random}.
Stacking means that another prediction model, in our case a simple decision tree, decides which of the two ensemble members makes the prediction about which instance.

\begin{table}[tbp]
  \centering
  \caption{MCC performance of models predicting timeouts. The models are trained 28 times with each solver as a target once.}
  \label{tab:timeout-prediction}
  ~\\[1em]
  \begin{tabular}{
    >{\raggedleft\arraybackslash}m{0.4\textwidth}
    >{\centering\arraybackslash}m{0.25\textwidth}
    >{\centering\arraybackslash}m{0.25\textwidth}
  }
    \toprule
    {Timeout Prediction Models} & {46 Instance Features Avg. MCC ($\pm$ std)} & {46\,+\,27 Runtime Feat. Avg. MCC ($\pm$ std)} \\
    \midrule
    Stacking (QDA\,+\,RF)                 & 0.6513 ($\pm$ 0.0458) & 0.9527 ($\pm$ 0.0292) \\[0.4ex]
    Quadratic Discriminant Analysis (QDA)                    & 0.2593 ($\pm$ 0.1310) & 0.9290 ($\pm$ 0.0339) \\[0.4ex]
    Random Forest (RF)         & 0.6607 ($\pm$ 0.0547) & 0.8530 ($\pm$ 0.0479) \\[0.4ex]
    AdaBoost                             & 0.5412 ($\pm$ 0.0612) & 0.8384 ($\pm$ 0.0444) \\[0.4ex]
    Decision Tree                         & 0.5980 ($\pm$ 0.0423) & 0.8059 ($\pm$ 0.0707) \\[0.4ex]
    Logistic Regression                   & 0.2031 ($\pm$ 0.0728) & 0.8052 ($\pm$ 0.1018) \\[0.4ex]
    $k$NN                   & 0.5108 ($\pm$ 0.0387) & 0.7885 ($\pm$ 0.1521) \\[0.4ex]
    Multilayer Perceptron                      & 0.1293 ($\pm$ 0.0718) & 0.7760 ($\pm$ 0.1408) \\[0.4ex]
    Support Vector Machine               & 0.0595 ($\pm$ 0.0554) & 0.7757 ($\pm$ 0.2149) \\[0.4ex]
    Naive Bayes                           & 0.1173 ($\pm$ 0.0872) & 0.7306 ($\pm$ 0.1394) \\
    \bottomrule
  \end{tabular}
\end{table}

% MI: table with cluster prediction performance: (merged with existing text)
Since runtime prediction is quite challenging, we discretize the runtimes.
To define prediction targets for each instance, we cluster the solver runtimes into $k$ clusters.
We denote this adapted solver scoring function which assigns a solver to its cluster for each instance with $s_k$ as defined in Equation~\eqref{eq:rankingeq}.
In preliminary experiments, the most \emph{useful} labels were produced with hierarchical clustering and a log-single-link criterion.
In our chosen hierarchical procedure, each non-timeout runtime starts in a separate interval.
We then gradually merge intervals with the smallest single-link logarithmic distance until the desired number of partitions is reached.
The clustering is instance-specific, i.e., the bounds are chosen to produce groups of solvers with similar performance for each instance.
Other clustering approaches we have tried include hierarchical clustering with mean, median, and full-link criteria, as well as $k$-means and spectral clustering.

We evaluated the performance of the best models from Table~\ref{tab:timeout-prediction} in predicting the such determined $s_k$.
Table~\ref{tab:cluster-prediction} shows their performance in predicting the $C_k$ clusters for $k \in \lbrace 3, 4, 5 \rbrace$.
While the second column shows the prediction performance of the models trained on the 46 instance features including the 27 known solver runtimes, the third column shows the prediction performance of the models trained on these features plus 27 features representing the known $C_k$ clustering labels of the known solver runtimes.
We observe that the models perform better when the clustering labels are included as features.
In addition, the prediction performance degrades rapidly as the number of clusters increases.
In all cases, the stacking ensemble performs best.

\begin{table}[tbp]
  \centering
  \caption{MCC performance of the best models predicting the $C_k$ clusterings.}
  \label{tab:cluster-prediction}
  \begin{tabular}{
    >{\raggedleft\arraybackslash}m{0.4\textwidth}
    >{\centering\arraybackslash}m{0.25\textwidth}
    >{\centering\arraybackslash}m{0.25\textwidth}
  }
    \toprule
    {$C_3$ Cluster Prediction Models} & {46\,+\,27 Runtime Feat. Avg. MCC ($\pm$ std)} & {46\,+\,27\,+\,27 Clus. Feat. Avg. MCC ($\pm$ std)} \\
    \midrule
    Stacking (QDA\,+\,RF)                 & 0.7464 ($\pm$ 0.0497) & 0.8380 ($\pm$ 0.0634) \\[0.4ex]
    Quadratic Discriminant Analysis (QDA)                     & 0.6903 ($\pm$ 0.0607) & 0.7738 ($\pm$ 0.0459) \\[0.4ex]
    Random Forest (RF)         & 0.7116 ($\pm$ 0.0385) & 0.7841 ($\pm$ 0.0469) \\[0.4ex]
    \midrule
    {$C_4$ Cluster Prediction Models} & {46\,+\,27 Runtime Feat. Avg. MCC ($\pm$ std)} & {46\,+\,27\,+\,27 Clus. Feat. Avg. MCC ($\pm$ std)} \\
    \midrule
    Stacking (QDA\,+\,RF)                 & 0.6562 ($\pm$ 0.0538) & 0.7366 ($\pm$ 0.0377) \\[0.4ex]
    Quadratic Discriminant Analysis (QDA)                     & 0.6007 ($\pm$ 0.0765) & 0.6872 ($\pm$ 0.0469) \\[0.4ex]
    Random Forest (RF)         & 0.6033 ($\pm$ 0.0455) & 0.6828 ($\pm$ 0.0527) \\[0.4ex]
    \midrule
    {$C_5$ Cluster Prediction Models} & {46\,+\,27 Runtime Feat. Avg. MCC ($\pm$ std)} & {46\,+\,27\,+\,27 Clus. Feat. Avg. MCC ($\pm$ std)} \\
    \midrule
    Stacking (QDA\,+\,RF)                 & 0.5835 ($\pm$ 0.0615) & 0.6396 ($\pm$ 0.0553) \\[0.4ex]
    Quadratic Discriminant Analysis (QDA)                      & 0.5508 ($\pm$ 0.0669) & 0.5881 ($\pm$ 0.0912) \\[0.4ex]
    Random Forest (RF)         & 0.5420 ($\pm$ 0.0459) & 0.5895 ($\pm$ 0.0475) \\
    \bottomrule
  \end{tabular}
\end{table}

Our final choice for the prediction model is the stacking ensemble of a quadratic discriminant analysis and a random forest.
Both of these models can learn non-linear relationships between the instance features and the runtime labels.
Fig.~\ref{fig:confusion-matrices} shows the confusion matrices of the stacking ensemble for predicting timeouts and $C_3$ clusters.
Both perform reasonably well, with the $C_3$ cluster prediction model performing slightly worse than the timeout prediction model.
Notably, for the $C_3$ prediction model, there is less confusion between the non-timeout labels (labels 1 and 2) and the timeout label (label 3) than between the non-timeout labels (labels 1 and 2).
This suggests that deciding on timeouts is easier than distinguishing between different groups of solvers.

\begin{figure}[tbp]
  \centering
  \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[height=4.123cm]{../plots/cm2labels.pdf}
    \caption{Timeout prediction.}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[height=4.228cm]{../plots/cm3labels.pdf}
    \caption{$C_3$ cluster prediction.}
  \end{subfigure}
  \caption{Confusion matrices for predicting timeouts and $C_3$ clusters.}
  \label{fig:confusion-matrices}
\end{figure}

To ensure that we obtained \emph{useful} labels, we evaluated whether the discretized labels still discriminate solvers and agree with the actual PAR-2 ranking.
Table~\ref{tab:ranking} shows the ranking of the solvers in the SAT Competition~2022 Anniversary Track~\cite{sat2022} according to the standard PAR-2 runtime score and according to our discretized $s_3$ score.
We observe that our ranking approach correctly decides almost all (about \SI{97.45}{\%}; $\sigma = \SI{3.68}{\%}$) solver pairs, i.e., which solver is faster among two given solvers.
In particular, the Spearman correlation of $s_3$ and PAR-2 ranking is about \SI{0.988}{}, which is very close to the optimal value of~1~\cite{de2016comparing}.

% TODO tf: Actual rankings with discretization
% * Correct ranking pairs in table below: 97.45%
% * Max. difference between wrong pairs 0.11
% * Spearman correlation of PAR2 and Discrete rank: 0.9862
\begin{table}[tbp]
  \centering
  \caption{Ranking of Anniversary-Track solvers with standard PAR-2 runtime and our discretized $s_3$ ranking.}
  \label{tab:ranking}
  ~\\[1em]
  \begin{tabular}{
    >{\centering\arraybackslash}m{0.09\textwidth}
    >{\centering\arraybackslash}m{0.15\textwidth}
    >{\centering\arraybackslash}m{0.09\textwidth}
    >{\centering\arraybackslash}m{0.15\textwidth}
    >{\raggedright\arraybackslash}m{0.01\textwidth}
    >{\raggedright\arraybackslash}m{0.314\textwidth}
  }
    \toprule
    \multicolumn{2}{c}{PAR-2} & \multicolumn{2}{c}{$s_3$} & & \multirow[c]{2}{*}{\vspace{-0.15cm}\hspace{-0.02cm}Solver} \\
    \cmidrule(r){1-2}\cmidrule(r){3-4}
    Rank &   Score & Rank &  Score & & \\
    \midrule
       1 & 2808.13 &    1 & 1.1717 & & kissat-mab-esa \\
       2 & 2812.93 &    2 & 1.1832 & & kissat-sc2022-bulky \\
       3 & 2835.25 &    3 & 1.1862 & & ekissat-mab-gb-db \\
       4 & 2835.59 &    4 & 1.1868 & & kissat-mab-ucb \\
       5 & 2836.92 &    5 & 1.1868 & & kissat-inc \\
       6 & 2845.19 &    6 & 1.1926 & & ekissat-mab-db-v1 \\
       7 & 2846.73 &    7 & 1.1930 & & kissat-mab-moss \\
       8 & 2857.67 &    8 & 1.1947 & & kissat-mab-hywalk \\
       9 & 2869.45 &    9 & 1.1998 & & kissat-sc2022-light \\
      10 & 2899.70 &   10 & 1.2164 & & kissat-els-v2 \\
      11 & 2953.59 &   11 & 1.2290 & & hkis-unsat \\
      12 & 2967.53 &   12 & 1.2347 & & kissat-adaptive-restart \\
      13 & 2976.56 &   13 & 1.2475 & & seqfrost-noextend \\
      14 & 3014.40 &   16 & 1.2645 & & kissat-els-v1 \\
      15 & 3017.73 &   14 & 1.2509 & & cadical-esa \\
      16 & 3036.83 &   15 & 1.2613 & & cadical-reorder \\
      17 & 3049.90 &   20 & 1.3648 & & cadical-rel-scavel \\
      18 & 3080.66 &   19 & 1.2965 & & kissat-relaxed \\
      19 & 3095.73 &   17 & 1.2815 & & cadical-dvdl-v1 \\
      20 & 3101.12 &   18 & 1.2856 & & cadical-dvdl-v2 \\
      21 & 3273.95 &   21 & 1.3786 & & glucose-reboot \\
      22 & 3290.90 &   25 & 1.4707 & & lstech-maple-hywalk \\
      23 & 3292.68 &   23 & 1.4478 & & lstech-maple \\
      24 & 3400.72 &   24 & 1.4693 & & slime-sc-2022-beta \\
      25 & 3412.11 &   26 & 1.5237 & & slime-sc-2022 \\
      26 & 3436.07 &   22 & 1.4161 & & hcad-v1-psids \\
      27 & 3506.32 &   27 & 1.5433 & & maple-lcmdistchrbt-dl-v3 \\
      28 & 4741.50 &   28 & 2.0581 & & isasat \\
    \bottomrule
  \end{tabular}
\end{table}

We also analyzed how runtime discretization affects the distinguishability of solver pairs.
Fig.~\ref{fig:solver-confusion-matrices} shows the confusion matrices of solver pairs with significant differences in their PAR-2 scores, timeouts, and $s_3$ scores.
According to a Wilcoxon signed-rank test with a level of $\alpha = 0.05$, \SI{87.83}{\%} of the solver pairs have significantly different scores after discretization, only a small decrease compared to \SI{89.95}{\%} before discretization.
These results support the usefulness of discretized runtimes for our framework.

% TODO tfu: Amount of significant solver pairs (Wilcoxon signed-rank, 5% alpha), 
% Amount of significant pairs:
% PAR-2:   684/756 = 0.9048
% timeout: 660/756 = 0.8730
% k=3:     662/756 = 0.8757
% k=4:     662/756 = 0.8757
% k=5:     662/756 = 0.8757
\begin{figure}[tbp]
  \centering
  \begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=3.9cm]{../plots/par2sigdiff.pdf}
    \caption{PAR-2 scores.}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=3.9cm]{../plots/timeoutsigdiff.pdf}
    \caption{Timeouts.}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=3.9cm]{../plots/s3sigdiff.pdf}
    \caption{$s_3$ scores.}
  \end{subfigure}
  \caption{Solver pairs with significant differences regarding the respective scorings are colored white. Non-significant differences are colored black. Solvers are ordered indentically to Tab.~\ref{tab:ranking}.}
  \label{fig:solver-confusion-matrices}
\end{figure}

\subsection{Implementation Details}

For reproducibility, our source code and data are available on GitHub (see footnotes in Section~\ref{sec:intro}).
Our code is implemented in \textsc{Python} using \emph{scikit-learn}~\cite{scikit-learn} for making predictions and \emph{gbd-tools}~\cite{IserS18} for retrieving SAT instances.


\section{Evaluation}
\label{sec:eval}

In this section, we evaluate our active learning framework.
First, we analyze and tune the different subroutines of our framework on the tuning dataset.
Next, we evaluate the best configurations on the full dataset.
Finally, we analyze the importance of different instance families for our framework.

\subsection{Hyperparameter Analysis}

\begin{figure}[tbp!]
  \centering
  \begin{subfigure}{1.0\textwidth}
    \resizebox{\textwidth}{!}{
      \graphicspath{{../plots/}}
      \input{../plots/anni_train_color_ranking.pgf}
    }
    \caption{Ranking approaches}
    \label{fig:annitraincolorranking}
  \end{subfigure}
  \\
  \vspace{0.2cm}
  \begin{subfigure}{1.0\textwidth}
    \resizebox{\textwidth}{!}{
      \graphicspath{{../plots/}}
      \input{../plots/anni_train_color_selection.pgf}
    }
    \caption{Selection approaches}
    \label{fig:annitraincolorselection}
  \end{subfigure}
  \\
  \vspace{0.2cm}
  \begin{subfigure}{1.0\textwidth}
    \resizebox{\textwidth}{!}{
      \graphicspath{{../plots/}}
      \input{../plots/anni_train_color_stopping.pgf}
    }
    \caption{Stopping criteria}
    \label{fig:annitraincolorstopping}
  \end{subfigure}
  \caption{
    $O_{\operatorname{rt}}$-$O_{\operatorname{acc}}$-diagrams comparing different hyperparameter instantiations of our active-learning framework on the hyperparameter-tuning dataset.
    The x-axis shows the ratio of total solver runtime on the sampled instances relative to all instances.
    The y-axis shows the ranking accuracy (cf. Section~\ref{sec:exdesign:eval}).
    Each line entails the front of Pareto-optimal configurations for the respective hyperparameter instantiation.
  }
  \label{fig:e2eallsolvers}
\end{figure}

Our experiments follow the evaluation framework introduced in Section~\ref{sec:exdesign:eval}.
Fig.~\ref{fig:e2eallsolvers} shows the performance of the approaches from Section~\ref{sec:exdesign:hyper} on $O_{\operatorname{rt}}$-$O_{\operatorname{acc}}$ plots for the hyperparameter-tuning dataset.
Evaluating a given configuration with Algorithm~\ref{alg:eval} returns a point $\left(O_{\operatorname{rt}},\, O_{\operatorname{acc}}\right)$.
We do not show intermediate results of the active learning procedure, but only the final results after stopping.
The plotted lines represent the best performing configurations per ranking approach (Fig.~\ref{fig:annitraincolorranking}), selection approach (Fig.~\ref{fig:annitraincolorselection}), and stopping criterion (Fig.~\ref{fig:annitraincolorstopping}).
In particular, we show the Pareto front, i.e., from all configurations that share a particular value of the plotted hyperparameter, we take the maximum ranking accuracy over all remaining hyperparameters \emph{not} shown in the corresponding plot.

Regarding ranking approaches (Fig.~\ref{fig:annitraincolorranking}), using the predicted $s_3$-induced run\-time-label ranking consistently outperforms the partially observed PAR-2 ranking for every possible value of the tradeoff parameter~$\delta$.
This result is expected, since selection decisions are not random.
For example, we might sample more instances of a family if it benefits the discrimination of solvers.
While the partially observed PAR-2 score is biased, the prediction model can account for this.

Regarding the selection approaches (Fig.~\ref{fig:annitraincolorselection}), uncertainty sampling performs best in most cases.
However, information-gain sampling is advantageous when runtime is strongly favored (small $\delta$; runtime fraction less than \SI{5}{\%}).
This result is consistent with our expectations:
Information-gain sampling selects instances that maximize the expected reduction in entropy.
This means that we sample instances that reveal similarities between solvers rather than differences, which helps to build a confident model quickly.
However, the method cannot select instances that are useful for distinguishing solvers later.
Random sampling performs reasonably well, but is outperformed by uncertainty sampling in all cases, demonstrating the benefit of actively selecting instances based on a prediction model.

Regarding the stopping criteria (Fig.~\ref{fig:annitraincolorstopping}), the ranking stopping criterion performs most consistently well.
When accuracy is strongly favored (very high $\delta$), the Wilcoxon stopping criterion performs better.
The subset-size stopping criterion performs reasonably well, but does not improve beyond a certain accuracy because it samples a fixed subset of instances.

\begin{figure}[tb!]
  \centering
  \begin{subfigure}{0.4775\textwidth}
    \resizebox{\textwidth}{!}{
      \graphicspath{{../plots/}}
      \input{../plots/anni_train_optimization_goal.pgf}
    }
    \caption{Runtime vs. Instances}
    \label{fig:annitrainoptgoalruntime}
  \end{subfigure}
  \begin{subfigure}{0.5125\textwidth}
    \resizebox{\textwidth}{!}{
      \graphicspath{{../plots/}}
      \input{../plots/anni_train_delta_acc.pgf}
    }
    \caption{Runtime vs. Accuracy}
    \label{fig:annitrainoptgoalacc}
  \end{subfigure}

  \caption{
    Scatter plot comparing different instantiations of the tradeoff parameter~$\delta$ for our active learning framework on the hyperparameter-tuning dataset.
    The x-axis shows the fraction of runtime $O_{\operatorname{rt}}$ of the sample, while the y-axes show the fraction of sampled instances and the ranking accuracy, respectively.
    The color indicates the weighting between different optimization objectives $\delta \in \left[0, 1\right]$.
    The larger $\delta$, the more we favor accuracy over runtime.
  }
  \label{fig:annitrainoptgoal}
\end{figure}

Fig.~\ref{fig:annitrainoptgoalruntime} shows an interesting consequence of weighting our optimization goals:
On the one hand, if we want to get a \emph{rough} estimate of a solver's performance quickly (low~$\delta$), the approaches favor selecting many \emph{easy} instances.
In particular, the fraction of sampled instances is larger than the fraction of runtime.
By having many observations, it is easier to build a model.
On the other hand, if we want to get a \emph{good} estimate of a solver's performance in a moderate amount of time (high~$\delta$), approaches favor selecting few, \emph{difficult} instances.
In particular, the fraction of instances is smaller than the fraction of runtime.
Furthermore, Fig.~\ref{fig:annitrainoptgoalacc} shows which values of $\delta$ make the most sense.
The range $\delta \in \left[0.2, 0.8\right]$ corresponds to the points with a runtime fraction between \SI{0.03} and \SI{0.22}.
We consider this region to be the most promising, analogous to the \emph{elbow} method in cluster analysis~\cite{kodinariya2013review}.

\subsection{Full-Dataset Evaluation}

After selecting the most promising hyperparameters, we run our active learning experiments on the complete Anniversary Track dataset (5355 instances).
The above range $\delta \in \left[0.2, 0.8\right]$ results in only two different configurations.
The best performing approach for $\delta \in \left[0.2, 0.7\right]$ uses the predicted runtime label ranking, information-gain sampling, and ranking stopping criterion.
It can predict the PAR-2 ranking of a new solver with \SI{90.48}{\%} accuracy ($O_{\operatorname{acc}}$) in only \SI{5.41}{\%} of the total evaluation time ($O_{\operatorname{rt}}$).
The best performing approach for $\delta \in (0.7, 0.8]$ uses the predicted runtime label ranking, uncertainty sampling, and ranking stopping criterion.
It can predict the PAR-2 ranking of a new solver with \SI{92.33}{\%} accuracy~($O_{\operatorname{acc}}$) in only \SI{10.35}{\%} of the total evaluation time~($O_{\operatorname{rt}}$).

\begin{table}[tp]
  \centering
  \caption{
    Performance comparison (on the full dataset) of the best performing AL approach (\emph{AL}), random sampling of the same runtime fraction with 1000 repetitions (\emph{Random}), and static selection of the instances most frequently sampled by active learning approaches (\emph{Most Frequent}).
  }
  \label{tab:fulldataset}
  \begin{tabular}{
    >{\arraybackslash}p{0.4\textwidth}
    S[table-format=2.2,table-column-width=0.15\textwidth]
    S[table-format=2.2,table-column-width=0.15\textwidth]
    S[table-format=2.2,table-column-width=0.15\textwidth]
  }
    \toprule
    {For $\delta \in \left[0.2, 0.7\right]$} & {AL} & {Random} & {Most Frequent} \\
    \midrule
    Sampled Runtime Fraction (\%) & 5.41 & 5.43 & 5.44 \\
    Sampled Instance Fraction (\%) & 26.53 & 5.43 & 27.75 \\
    Ranking Accuracy (\%) & 90.48 & 88.54 & 81.08 \\
    \midrule
    {For $\delta \in (0.7, 0.8]$}  & {AL} & {Random} & {Most Frequent} \\
    \midrule
    Sampled Runtime Fraction (\%) & 10.35 & 10.37 & 10.37 \\
    Sampled Instance Fraction (\%) & 5.24 & 10.37 & 36.96 \\
    Ranking Accuracy (\%) & 92.33 & 91.61 & 84.52 \\
    \bottomrule
  \end{tabular}
\end{table}

Table~\ref{tab:fulldataset} shows how the two active learning approaches (column \emph{AL}) compare to two static baselines:
\emph{Random} samples instances until it reaches roughly the same fraction of runtime as the AL benchmark sets.
We repeat the sampling 1000 times and report the average results.
\emph{Most Frequent} uses a static benchmark set consisting of the instances most frequently sampled by our active learning approach.
In particular, we consider the average sampling frequency across all solvers and Pareto-optimal active learning approaches.

Both of our AL approaches outperform random sampling.
However, the performance differences are not significant with respect to a Wilcoxon signed-rank test with $\alpha = 0.05$, and also depend on the fraction of the runtime sampled (cf.~Fig.~\ref{fig:annitraincolorselection}).
However, a clear advantage of our approach is that it indicates when to stop adding new instances, depending on the tradeoff parameter~$\delta$.
While the active learning results on the full dataset are not as strong as on the smaller tuning dataset, they still show the benefit of making the benchmark selection dependent on the solvers to be distinguished.

A static benchmark utilizing the most frequently AL-sampled instances exhibits suboptimal performance when compared to active learning and random sampling.
This outcome is somewhat anticipated, given that the static benchmark fails to reflect an appropriate balance of instance families.
For instance, families whose instances are uniformly randomly selected by AL, e.g., for different solvers, appear less frequently in this benchmark than families where some instances are sampled more often than others.
% MI: What? I don't understand this sentence.

\subsection{Instance Frequency and Instance-Family Importance}
\label{sec:eval:instance}

\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.6\textwidth]{../plots/instoccs.pdf}
  \caption{Frequency with which individual problem instances are selected within all active-learning benchmarks. Instances that are chosen more frequently are intuitively more important for distinguishing solver runtimes.}
  \label{fig:instoccs}
\end{figure}

\begin{figure}[tb]
  \centering
  \resizebox{0.85\textwidth}{!}{
    \graphicspath{{../plots/}}
    \input{../plots/anni_final_families.pgf}
  }
  \caption{
    Scatter plot illustrating the relative importance of different instance families for ranking. The x-axis depicts the frequency of instance families in the dataset, while the y-axis represents the average frequency of instance families in the samples selected by active learning. The dashed line represents families that occur with the same frequency in the dataset and samples.
  }
  \label{fig:annifinalfamilies}
\end{figure}

Fig.~\ref{fig:instoccs} illustrates the distribution of the frequency with which individual instances are selected across all active learning benchmarks.
It can be observed that a small proportion of instances is selected with great frequency, while most instances are selected on only a few occasions.
This indicates that there is no fixed subset of instances that can perfectly distinguish all solvers, which highlights the value of employing an active-learning-based strategy.

The selection decisions of our approach also reveal the importance of different instance families to our framework.
Fig.~\ref{fig:annifinalfamilies} illustrates the occurrence of instance families within the dataset and the benchmarks created by active learning.
We use the best performing configurations for all values of $\delta \in \left[0, 1\right]$ and examine the selection decisions made by the active learning approach on the SAT Competition~2022 Anniversary Track dataset~\cite{sat2022}.
While most families exhibit the same fraction in the dataset and the sampled benchmarks, a few outliers require further discussion.
Instances of the families \emph{fpga}, \emph{quasigroup-completion}, and \emph{planning} are particularly useful for distinguishing solvers within our framework.
These instances are selected in greater proportion than in the full dataset.
In contrast, instances of the largest family, i.e., \emph{hardware-verification}, appear with approximately the same fraction in the dataset and the sampled benchmarks.
Finally, instances of the family \emph{cryptography} are less important in distinguishing solvers than their substantial representation in the dataset would suggest.
One possible explanation for this discrepancy is that these instances are highly similar, such that a small fraction of them is sufficient to estimate a solver's performance on all of them.

\section{Conclusions and Future Work}

In this work, we have addressed the \emph{New-Solver Problem}:
Given a new solver, we aim to determine its ranking relative to competitors.
Our approach provides accurate ranking predictions while requiring significantly less runtime than a complete evaluation on a given benchmark set.
On data from the SAT~Competition~2022 Anniversary Track, we can determine a new solver's PAR-2 ranking with about \SI{92}{\%} accuracy while only requiring \SI{10}{\%} of the full-evaluation time.
We have evaluated several ranking algorithms, instance-selection approaches, and stopping criteria within our sequential active learning framework.
We have also looked at which instance families are the most prevalent in selection decisions.

Future work could compare more sub-routines for ranking, instance selection, and stopping.
Further, our evaluation framework can be used for other computation-intensive problems, like other $\mathcal{NP}$-complete problems than SAT.
Such problems share most of the relevant properties of SAT, like established instance features, a complete benchmark is expensive, and traditional benchmark selection requires expert knowledge.

From a technical perspective, we could formulate the problem of runtime discretization as an optimization problem, rather than addressing it empirically.
Further, a major shortcoming of our current approach is the lack of parallelization, selecting instances one at a time.
Benchmarking on a computing cluster with $n$ cores benefits from having batches of $n$ instances.
However, bigger batch sizes $n$ impede \emph{active learning}.
Also, it is unclear how to synchronize instance selection and updates of the prediction model without wasting too much runtime.

To perform benchmarking on a computer cluster with multiple cores, it is beneficial to work with batches of multiple instances.
In contrast, our approach is limited to benchmarking with a single instance at a time.
In future work, we intend to investigate models that learn from partial labels, which can be extracted from larger batch sizes in a parallel approach that synchronizes instance selection and model updates.

\backmatter

\section*{Declarations}

\bmhead{Funding}

This work was supported by the Ministry of Science, Research and the Arts Baden-WÃ¼rttemberg, project \emph{Algorithm Engineering for the Scalability Challenge (AESC)}.

\bmhead{Competing interests}

The authors have no competing interests to declare that are relevant to the content of this article.

\bmhead{Availability of data and materials}

All experimental data are available online at \url{https://github.com/mathefuchs/al-for-sat-solver-benchmarking-data}.

\bmhead{Code availability}

The code is available online at \url{https://github.com/mathefuchs/al-for-sat-solver-benchmarking}.

\bibliography{literature}

\end{document}
